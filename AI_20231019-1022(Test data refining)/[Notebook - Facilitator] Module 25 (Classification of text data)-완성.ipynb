{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [재난예측기 - tweets 데이터 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 방법1 :  파이썬 코딩( tfidf 직접 구하기 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### < 기계학습 준비하기 > \n",
    "#### 1. 라이브러리 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-1. 데이터셋 준비하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>choose_one</th>\n",
       "      <th>relevance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text choose_one  relevance\n",
       "0                 Just happened a terrible car crash   Relevant          1\n",
       "1  Our Deeds are the Reason of this #earthquake M...   Relevant          1\n",
       "2  Heard about #earthquake is different cities, s...   Relevant          1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw = pd.read_csv('[Dataset]_Module25_disasters_social_media.csv',encoding='ISO-8859-1')\n",
    "\n",
    "df_choice_one = df_raw[df_raw['choose_one'] != \"Can't Decide\"]\n",
    "df = df_choice_one[['text', 'choose_one']].copy()\n",
    "relevance = {'Relevant':1,'Not Relevant':0}\n",
    "df['relevance'] = df.choose_one.map(relevance) \n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2. 데이터 전처리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'just': 453,\n",
       " 'happened': 31,\n",
       " 'terrible': 14,\n",
       " 'car': 133,\n",
       " 'crash': 166,\n",
       " 'our': 150,\n",
       " 'deeds': 2,\n",
       " 'reason': 29,\n",
       " 'this': 702,\n",
       " 'earthquake': 72,\n",
       " 'may': 113,\n",
       " 'allah': 11,\n",
       " 'forgive': 4,\n",
       " 'us': 176,\n",
       " 'all': 381,\n",
       " 'heard': 56,\n",
       " 'about': 312,\n",
       " 'different': 17,\n",
       " 'cities': 11,\n",
       " 'stay': 40,\n",
       " 'safe': 15,\n",
       " 'everyone': 73,\n",
       " 'there': 264,\n",
       " 'forest': 105,\n",
       " 'fire': 363,\n",
       " 'at': 745,\n",
       " 'spot': 30,\n",
       " 'pond': 7,\n",
       " 'geese': 1,\n",
       " 'fleeing': 3,\n",
       " 'across': 27,\n",
       " 'street': 37,\n",
       " 'i': 2486,\n",
       " 'cannot': 15,\n",
       " 'save': 57,\n",
       " 'them': 165,\n",
       " 'near': 84,\n",
       " 'la': 25,\n",
       " 'ronge': 1,\n",
       " 'sask': 1,\n",
       " 'canada': 17,\n",
       " 'residents': 13,\n",
       " 'asked': 14,\n",
       " 'shelter': 9,\n",
       " 'in': 2805,\n",
       " 'place': 34,\n",
       " 'being': 121,\n",
       " 'notified': 1,\n",
       " 'by': 768,\n",
       " 'officers': 11,\n",
       " 'no': 400,\n",
       " 'other': 66,\n",
       " 'evacuation': 67,\n",
       " 'or': 294,\n",
       " 'orders': 12,\n",
       " 'expected': 18,\n",
       " '13': 33,\n",
       " '000': 6,\n",
       " 'people': 282,\n",
       " 'receive': 3,\n",
       " 'wildfires': 16,\n",
       " 'california': 159,\n",
       " 'got': 160,\n",
       " 'sent': 14,\n",
       " 'photo': 64,\n",
       " 'from': 613,\n",
       " 'ruby': 2,\n",
       " 'alaska': 9,\n",
       " 'as': 485,\n",
       " 'smoke': 66,\n",
       " 'pours': 1,\n",
       " 'into': 239,\n",
       " 'school': 86,\n",
       " 'rockyfire': 5,\n",
       " 'update': 52,\n",
       " 'hwy': 10,\n",
       " '20': 31,\n",
       " 'closed': 27,\n",
       " 'both': 31,\n",
       " 'directions': 1,\n",
       " 'due': 53,\n",
       " 'lake': 18,\n",
       " 'county': 53,\n",
       " 'cafire': 2,\n",
       " 'apocalypse': 56,\n",
       " 'lighting': 5,\n",
       " 'spokane': 2,\n",
       " 'flood': 85,\n",
       " 'disaster': 219,\n",
       " 'heavy': 26,\n",
       " 'rain': 62,\n",
       " 'causes': 21,\n",
       " 'flash': 27,\n",
       " 'flooding': 64,\n",
       " 'streets': 11,\n",
       " 'manitou': 1,\n",
       " 'colorado': 21,\n",
       " 'springs': 6,\n",
       " 'areas': 12,\n",
       " 'typhoon': 79,\n",
       " 'soudelor': 36,\n",
       " 'kills': 58,\n",
       " '28': 7,\n",
       " 'china': 52,\n",
       " 'taiwan': 18,\n",
       " 'we': 445,\n",
       " 're': 196,\n",
       " 'shaking': 4,\n",
       " 's': 1272,\n",
       " 'an': 356,\n",
       " 'm': 531,\n",
       " 'on': 1236,\n",
       " 'top': 80,\n",
       " 'hill': 8,\n",
       " 'can': 354,\n",
       " 'see': 149,\n",
       " 'woods': 3,\n",
       " 'emergency': 229,\n",
       " 'happening': 18,\n",
       " 'now': 308,\n",
       " 'building': 44,\n",
       " 'afraid': 8,\n",
       " 'that': 844,\n",
       " 'tornado': 43,\n",
       " 'coming': 71,\n",
       " 'area': 60,\n",
       " 'three': 50,\n",
       " 'died': 47,\n",
       " 'heat': 70,\n",
       " 'wave': 52,\n",
       " 'so': 429,\n",
       " 'far': 43,\n",
       " 'haha': 26,\n",
       " 'south': 44,\n",
       " 'tampa': 6,\n",
       " 'getting': 75,\n",
       " 'flooded': 6,\n",
       " 'hah': 3,\n",
       " 'wait': 37,\n",
       " 'second': 37,\n",
       " 'live': 90,\n",
       " 'what': 310,\n",
       " 'am': 92,\n",
       " 'gonna': 67,\n",
       " 'do': 246,\n",
       " 'raining': 4,\n",
       " 'florida': 10,\n",
       " 'tampabay': 1,\n",
       " '18': 26,\n",
       " '19': 10,\n",
       " 'days': 41,\n",
       " 've': 109,\n",
       " 'lost': 32,\n",
       " 'count': 4,\n",
       " 'bago': 2,\n",
       " 'myanmar': 24,\n",
       " 'arrived': 13,\n",
       " 'damage': 72,\n",
       " 'bus': 49,\n",
       " '80': 5,\n",
       " 'multi': 5,\n",
       " 'breaking': 57,\n",
       " 'they': 323,\n",
       " 'd': 104,\n",
       " 'probably': 33,\n",
       " 'still': 180,\n",
       " 'show': 62,\n",
       " 'more': 323,\n",
       " 'life': 136,\n",
       " 'than': 188,\n",
       " 'arsenal': 8,\n",
       " 'did': 108,\n",
       " 'yesterday': 23,\n",
       " 'eh': 7,\n",
       " 'hey': 32,\n",
       " 'up': 468,\n",
       " 'man': 152,\n",
       " 'love': 146,\n",
       " 'fruits': 2,\n",
       " 'summer': 71,\n",
       " 'lovely': 12,\n",
       " 'fast': 28,\n",
       " 'nice': 19,\n",
       " 'hat': 37,\n",
       " 'goooooooaaaaaal': 1,\n",
       " 'don': 231,\n",
       " 't': 7442,\n",
       " 'like': 492,\n",
       " 'cold': 22,\n",
       " 'ridiculous': 4,\n",
       " 'london': 21,\n",
       " 'cool': 41,\n",
       " 'skiing': 1,\n",
       " 'wonderful': 7,\n",
       " 'day': 158,\n",
       " 'nooooooooo': 1,\n",
       " 'looooool': 1,\n",
       " 'way': 113,\n",
       " 'eat': 19,\n",
       " 'was': 554,\n",
       " 'nyc': 15,\n",
       " 'last': 126,\n",
       " 'week': 60,\n",
       " 'girlfriend': 6,\n",
       " 'cooool': 1,\n",
       " 'pasta': 3,\n",
       " 'tell': 41,\n",
       " 'me': 444,\n",
       " 'end': 54,\n",
       " 'awesome': 23,\n",
       " 'birmingham': 6,\n",
       " 'wholesale': 6,\n",
       " 'market': 52,\n",
       " 'ablaze': 42,\n",
       " 'bbc': 32,\n",
       " 'news': 289,\n",
       " 'breaks': 6,\n",
       " 'out': 409,\n",
       " 'http': 6153,\n",
       " 'co': 6799,\n",
       " 'irwqcezweu': 1,\n",
       " 'sunkxssedharry': 1,\n",
       " 'will': 372,\n",
       " 'wear': 6,\n",
       " 'shorts': 2,\n",
       " 'for': 1243,\n",
       " 'race': 5,\n",
       " 'bbcmtd': 1,\n",
       " 'markets': 9,\n",
       " 'lhyxeohy6c': 1,\n",
       " 'always': 67,\n",
       " 'try': 37,\n",
       " 'bring': 28,\n",
       " 'metal': 18,\n",
       " 'rt': 165,\n",
       " 'yao1e0xngw': 1,\n",
       " 'africanbaze': 1,\n",
       " 'nigeria': 12,\n",
       " 'flag': 29,\n",
       " 'set': 66,\n",
       " 'aba': 21,\n",
       " '2nndbgwyei': 1,\n",
       " 'previouslyondoyintv': 1,\n",
       " 'makinwaâ': 1,\n",
       " 'ã': 1633,\n",
       " 'âªs': 125,\n",
       " 'marriage': 2,\n",
       " 'crisis': 33,\n",
       " 'sets': 10,\n",
       " 'nigerian': 23,\n",
       " 'twitter': 48,\n",
       " 'cmghxba2xi': 1,\n",
       " 'crying': 12,\n",
       " 'plus': 9,\n",
       " 'side': 32,\n",
       " 'look': 95,\n",
       " 'sky': 20,\n",
       " 'night': 82,\n",
       " 'qqsmshaj3n': 1,\n",
       " 'phdsquares': 1,\n",
       " 'mufc': 2,\n",
       " 'built': 10,\n",
       " 'much': 86,\n",
       " 'hype': 4,\n",
       " 'around': 61,\n",
       " 'new': 327,\n",
       " 'acquisitions': 2,\n",
       " 'but': 436,\n",
       " 'doubt': 6,\n",
       " 'epl': 1,\n",
       " 'season': 24,\n",
       " 'inec': 2,\n",
       " 'office': 16,\n",
       " 'abia': 2,\n",
       " '3imaomknna': 1,\n",
       " 'barbados': 1,\n",
       " 'bridgetown': 1,\n",
       " 'jamaica': 5,\n",
       " 'â': 535,\n",
       " 'two': 152,\n",
       " 'cars': 26,\n",
       " 'santa': 6,\n",
       " 'cruz': 10,\n",
       " 'head': 59,\n",
       " 'st': 41,\n",
       " 'elizabeth': 2,\n",
       " 'police': 198,\n",
       " 'superintende': 1,\n",
       " 'wdueaj8q4j': 1,\n",
       " 'lord': 27,\n",
       " 'check': 64,\n",
       " 'these': 70,\n",
       " 'roi2nsmejj': 3,\n",
       " '3tj8zjin21': 3,\n",
       " 'yduixefipe': 3,\n",
       " 'lxtjc87kls': 3,\n",
       " 'nsfw': 6,\n",
       " 'psa': 5,\n",
       " 'iâ': 23,\n",
       " 'âªm': 11,\n",
       " 'splitting': 1,\n",
       " 'personalities': 1,\n",
       " 'techies': 1,\n",
       " 'follow': 30,\n",
       " 'ablaze_co': 1,\n",
       " 'burners': 1,\n",
       " 'outside': 31,\n",
       " 'alive': 19,\n",
       " 'dead': 137,\n",
       " 'inside': 35,\n",
       " 'had': 162,\n",
       " 'time': 166,\n",
       " 'visiting': 2,\n",
       " 'cfc': 2,\n",
       " 'ancop': 1,\n",
       " 'site': 40,\n",
       " 'thanks': 42,\n",
       " 'tita': 2,\n",
       " 'vida': 2,\n",
       " 'taking': 25,\n",
       " 'care': 40,\n",
       " 'soooo': 2,\n",
       " 'pumped': 1,\n",
       " 'southridgelife': 1,\n",
       " 'wanted': 23,\n",
       " 'chicago': 27,\n",
       " 'with': 798,\n",
       " 'preaching': 2,\n",
       " 'not': 430,\n",
       " 'hotel': 11,\n",
       " 'o9qknbfofx': 1,\n",
       " 'gained': 6,\n",
       " '3': 201,\n",
       " 'followers': 8,\n",
       " 'know': 157,\n",
       " 'your': 429,\n",
       " 'stats': 3,\n",
       " 'grow': 9,\n",
       " 'tiyulif5c6': 1,\n",
       " 'west': 42,\n",
       " 'burned': 57,\n",
       " 'thousands': 26,\n",
       " 'alone': 24,\n",
       " 'vl5tbr3wbr': 1,\n",
       " 'perfect': 13,\n",
       " 'tracklist': 1,\n",
       " 'leave': 31,\n",
       " 'beware': 6,\n",
       " 'world': 147,\n",
       " 'sierra': 1,\n",
       " 'leone': 1,\n",
       " 'amp': 510,\n",
       " 'guap': 1,\n",
       " 'burning': 168,\n",
       " 'turban': 1,\n",
       " 'diva': 2,\n",
       " 'hodwosamws': 1,\n",
       " 'via': 325,\n",
       " 'etsy': 1,\n",
       " 'first': 167,\n",
       " 'retainers': 1,\n",
       " 'quite': 12,\n",
       " 'weird': 13,\n",
       " 'better': 50,\n",
       " 'get': 335,\n",
       " 'used': 43,\n",
       " 'have': 511,\n",
       " 'every': 73,\n",
       " 'single': 18,\n",
       " 'next': 67,\n",
       " 'year': 143,\n",
       " 'least': 51,\n",
       " 'diss': 6,\n",
       " 'song': 46,\n",
       " 'take': 103,\n",
       " '1': 176,\n",
       " 'thing': 56,\n",
       " 'run': 63,\n",
       " 'smh': 19,\n",
       " 'eye': 27,\n",
       " 'opener': 2,\n",
       " 'though': 41,\n",
       " '2': 310,\n",
       " 'game': 61,\n",
       " 'cyhitheprynce': 2,\n",
       " 'deputies': 4,\n",
       " 'shot': 33,\n",
       " 'before': 81,\n",
       " 'brighton': 3,\n",
       " 'home': 106,\n",
       " 'gwnrhmso8k': 1,\n",
       " 'wife': 22,\n",
       " 'six': 12,\n",
       " 'years': 110,\n",
       " 'jail': 5,\n",
       " 'setting': 15,\n",
       " 'niece': 3,\n",
       " 'ev1ahoucza': 1,\n",
       " 'victim': 19,\n",
       " 'dies': 21,\n",
       " 'herself': 5,\n",
       " '16': 41,\n",
       " 'old': 133,\n",
       " 'girl': 60,\n",
       " 'burn': 20,\n",
       " 'injuries': 63,\n",
       " 'ablazeâ': 1,\n",
       " '_': 529,\n",
       " 'uk8hnrboob': 1,\n",
       " 'superintendent': 1,\n",
       " 'lanford': 1,\n",
       " 'salmon': 3,\n",
       " 'has': 341,\n",
       " 'r': 38,\n",
       " 'vplr5hka2u': 1,\n",
       " 'sxhw2tnnlf': 1,\n",
       " 'arsonist': 28,\n",
       " 'deliberately': 1,\n",
       " 'black': 100,\n",
       " 'church': 11,\n",
       " 'north': 37,\n",
       " 'carolinaã': 1,\n",
       " 'pcxarbh9an': 1,\n",
       " 'noches': 1,\n",
       " 'el': 16,\n",
       " 'bestia': 1,\n",
       " 'alexis_sanchez': 1,\n",
       " 'happy': 35,\n",
       " 'teammates': 1,\n",
       " 'training': 19,\n",
       " 'hard': 27,\n",
       " 'goodnight': 1,\n",
       " 'gunners': 1,\n",
       " 'uc4j4jhvgr': 1,\n",
       " 'kurds': 1,\n",
       " 'trampling': 1,\n",
       " 'turkmen': 2,\n",
       " 'later': 18,\n",
       " 'while': 75,\n",
       " 'others': 25,\n",
       " 'vandalized': 2,\n",
       " 'offices': 3,\n",
       " 'front': 25,\n",
       " 'diyala': 1,\n",
       " '4izfdyc3cg': 1,\n",
       " 'truck': 75,\n",
       " 'r21': 1,\n",
       " 'voortrekker': 1,\n",
       " 'ave': 21,\n",
       " 'tambo': 1,\n",
       " 'intl': 2,\n",
       " 'cargo': 4,\n",
       " 'section': 5,\n",
       " '8kscqkfkkf': 1,\n",
       " 'hearts': 6,\n",
       " 'city': 85,\n",
       " 'gift': 6,\n",
       " 'skyline': 2,\n",
       " 'kiss': 4,\n",
       " 'upon': 22,\n",
       " 'lips': 1,\n",
       " 'https': 618,\n",
       " 'cyompz1a0z': 1,\n",
       " 'tonight': 60,\n",
       " 'los': 6,\n",
       " 'angeles': 6,\n",
       " 'expecting': 4,\n",
       " 'ig': 6,\n",
       " 'fb': 5,\n",
       " 'be': 587,\n",
       " 'filled': 4,\n",
       " 'sunset': 8,\n",
       " 'shots': 20,\n",
       " 'peeps': 3,\n",
       " 'icsjgz9te1': 1,\n",
       " 'climate': 28,\n",
       " 'energy': 10,\n",
       " '9fxmn0l0bd': 1,\n",
       " 'myself': 34,\n",
       " '6vme7p5xhc': 1,\n",
       " 'revel': 1,\n",
       " 'yours': 32,\n",
       " 'wmv': 2,\n",
       " 'videos': 10,\n",
       " 'means': 20,\n",
       " 'mac': 10,\n",
       " 'farewell': 1,\n",
       " 'en': 5,\n",
       " 'route': 13,\n",
       " 'dvd': 7,\n",
       " 'gtxrwm': 1,\n",
       " 'progressive': 1,\n",
       " 'greetings': 1,\n",
       " 'month': 20,\n",
       " 'students': 16,\n",
       " 'would': 214,\n",
       " 'their': 129,\n",
       " 'pens': 2,\n",
       " 'torch': 5,\n",
       " 'publications': 1,\n",
       " '9fxpixqujt': 1,\n",
       " 'ctvtoronto': 1,\n",
       " 'bins': 1,\n",
       " 'field': 12,\n",
       " 'house': 76,\n",
       " 'wer': 1,\n",
       " 'flames': 76,\n",
       " 'went': 48,\n",
       " 'rite': 2,\n",
       " 'hydro': 1,\n",
       " 'pole': 4,\n",
       " 'wonder': 24,\n",
       " 'him': 106,\n",
       " 'nowplaying': 29,\n",
       " 'alfons': 1,\n",
       " '2015': 118,\n",
       " 'puls': 1,\n",
       " 'radio': 27,\n",
       " 'pulsradio': 1,\n",
       " 'aa5bjgwfdv': 1,\n",
       " 'rene': 2,\n",
       " 'jacinta': 1,\n",
       " 'secret': 20,\n",
       " '2k13': 1,\n",
       " 'fallen': 5,\n",
       " 'skies': 3,\n",
       " 'edit': 3,\n",
       " 'mar': 2,\n",
       " '30': 60,\n",
       " '2013': 17,\n",
       " '7mlmsuzv1z': 1,\n",
       " 'rahm': 1,\n",
       " 'let': 116,\n",
       " 'hope': 78,\n",
       " 'hall': 4,\n",
       " 'builds': 4,\n",
       " 'giant': 29,\n",
       " 'wooden': 2,\n",
       " 'mayoral': 1,\n",
       " 'effigy': 1,\n",
       " '100': 31,\n",
       " 'feet': 18,\n",
       " 'tall': 2,\n",
       " 'kfo2mksn6y': 1,\n",
       " 'john_k': 1,\n",
       " 'navista7': 1,\n",
       " 'steve': 12,\n",
       " 'fires': 156,\n",
       " 'here': 119,\n",
       " 'something': 49,\n",
       " 'else': 30,\n",
       " 'tinderbox': 1,\n",
       " 'clown': 1,\n",
       " 'hood': 2,\n",
       " 'news24680': 1,\n",
       " 'ian': 4,\n",
       " 'buff': 1,\n",
       " 'magnitude': 4,\n",
       " 'av2jsjfftc': 1,\n",
       " 'edm': 15,\n",
       " 'nxwestmidlands': 1,\n",
       " 'huge': 32,\n",
       " 'rwzbfvnxer': 1,\n",
       " 'philippaeilhart': 1,\n",
       " 'dhublath': 1,\n",
       " 'hurt': 9,\n",
       " 'her': 196,\n",
       " 'eyes': 35,\n",
       " 'insulted': 1,\n",
       " 'anger': 7,\n",
       " 'does': 66,\n",
       " 'talk': 29,\n",
       " 'go': 140,\n",
       " 'until': 62,\n",
       " 'make': 122,\n",
       " 'work': 100,\n",
       " 'kids': 45,\n",
       " 'cuz': 8,\n",
       " 'bicycle': 3,\n",
       " 'accident': 125,\n",
       " 'split': 4,\n",
       " 'testicles': 1,\n",
       " 'impossible': 5,\n",
       " 'michael': 21,\n",
       " 'father': 9,\n",
       " '24': 22,\n",
       " 'w': 115,\n",
       " 'nashvilletraffic': 1,\n",
       " 'traffic': 35,\n",
       " 'moving': 13,\n",
       " '8m': 1,\n",
       " 'slower': 4,\n",
       " 'usual': 5,\n",
       " '0ghk693egj': 1,\n",
       " 'center': 30,\n",
       " 'lane': 10,\n",
       " 'blocked': 16,\n",
       " 'santaclara': 1,\n",
       " '101': 8,\n",
       " 'nb': 3,\n",
       " 'great': 83,\n",
       " 'america': 37,\n",
       " 'pkwy': 2,\n",
       " 'bayarea': 1,\n",
       " 'pmlohzurwr': 1,\n",
       " 'cleared': 8,\n",
       " 'paturnpike': 1,\n",
       " 'patp': 1,\n",
       " 'eb': 5,\n",
       " 'between': 23,\n",
       " 'pa': 5,\n",
       " 'cranberry': 1,\n",
       " 'slow': 7,\n",
       " 'back': 160,\n",
       " 'sl0oqn0vyr': 1,\n",
       " 'gkye6gjtk5': 1,\n",
       " 'personalinjury': 1,\n",
       " 'read': 84,\n",
       " 'advice': 5,\n",
       " 'solicitor': 2,\n",
       " 'help': 93,\n",
       " 'otleyhour': 1,\n",
       " 'self': 54,\n",
       " 'curling': 1,\n",
       " 'wand': 1,\n",
       " 'swear': 9,\n",
       " 'someone': 63,\n",
       " 'needs': 27,\n",
       " 'away': 52,\n",
       " 'cuase': 1,\n",
       " 'prone': 4,\n",
       " 'stlouis': 1,\n",
       " 'caraccidentlawyer': 1,\n",
       " 'speeding': 1,\n",
       " 'among': 17,\n",
       " 'teen': 17,\n",
       " 'accidents': 8,\n",
       " 'k4zomof319': 1,\n",
       " 's2kxvm0cba': 1,\n",
       " 'teeâ': 1,\n",
       " 'hate': 31,\n",
       " 'badging': 1,\n",
       " 'reported': 21,\n",
       " 'motor': 5,\n",
       " 'vehicle': 27,\n",
       " 'curry': 1,\n",
       " 'herman': 1,\n",
       " 'rd': 29,\n",
       " 'stephenson': 1,\n",
       " 'involving': 18,\n",
       " 'overturned': 2,\n",
       " 'please': 99,\n",
       " 'use': 53,\n",
       " 'ybjezkurw1': 1,\n",
       " 'bigrigradio': 1,\n",
       " 'awareness': 2,\n",
       " 'recorder': 2,\n",
       " 'zeroedgeã': 1,\n",
       " 'dual': 4,\n",
       " 'lens': 1,\n",
       " 'camera': 8,\n",
       " 'driving': 26,\n",
       " 'history': 37,\n",
       " 'camcorder': 1,\n",
       " 'large': 25,\n",
       " 'kkfasjv6cj': 1,\n",
       " '77': 6,\n",
       " 'mile': 13,\n",
       " 'marker': 4,\n",
       " '31': 11,\n",
       " 'mooresville': 2,\n",
       " 'iredell': 2,\n",
       " 'ramp': 1,\n",
       " '8': 88,\n",
       " '6': 81,\n",
       " 'pm': 87,\n",
       " 'coincidence': 1,\n",
       " 'curse': 2,\n",
       " 'unresolved': 1,\n",
       " 'secrets': 10,\n",
       " 'past': 56,\n",
       " '7vg8df9ple': 1,\n",
       " 'sleepjunkies': 1,\n",
       " 'sleeping': 22,\n",
       " 'pills': 1,\n",
       " 'double': 21,\n",
       " 'risk': 21,\n",
       " '7s9nm1fict': 1,\n",
       " 'knew': 13,\n",
       " 'gon': 4,\n",
       " 'happen': 22,\n",
       " 'ysxun5vceh': 1,\n",
       " 'traffic_southe': 1,\n",
       " 'roadpol_east': 1,\n",
       " 'a27': 1,\n",
       " 'lewes': 1,\n",
       " 'kingston': 1,\n",
       " 'roundabout': 1,\n",
       " 'rather': 15,\n",
       " 'a283': 1,\n",
       " 'n': 49,\n",
       " 'cabrillo': 1,\n",
       " 'magellan': 1,\n",
       " 'av': 5,\n",
       " 'mir': 1,\n",
       " '08': 62,\n",
       " '06': 34,\n",
       " '15': 74,\n",
       " '11': 87,\n",
       " '03': 8,\n",
       " '58': 3,\n",
       " '40': 56,\n",
       " 'congestion': 1,\n",
       " 'pastor': 1,\n",
       " 'scene': 23,\n",
       " 'who': 270,\n",
       " 'owner': 15,\n",
       " 'range': 4,\n",
       " 'rover': 2,\n",
       " 'sakuma_en': 1,\n",
       " 'pretend': 4,\n",
       " 'feel': 86,\n",
       " 'certain': 11,\n",
       " 'feeling': 28,\n",
       " 'become': 29,\n",
       " 'genuine': 5,\n",
       " 'hei': 1,\n",
       " 'darker': 1,\n",
       " 'manga': 2,\n",
       " 'anime': 2,\n",
       " 'legal': 7,\n",
       " 'medical': 14,\n",
       " 'referral': 1,\n",
       " 'service': 63,\n",
       " '1800_injured': 1,\n",
       " 'call': 61,\n",
       " '800': 3,\n",
       " '465': 1,\n",
       " '87332': 1,\n",
       " 'slipandfall': 1,\n",
       " 'dogbite': 1,\n",
       " 'mom': 32,\n",
       " 'didn': 36,\n",
       " 'wished': 1,\n",
       " 'why': 175,\n",
       " 'some': 179,\n",
       " 'spilt': 1,\n",
       " 'mayonnaise': 1,\n",
       " 'over': 257,\n",
       " 'horrible': 49,\n",
       " 'sunday': 18,\n",
       " 'finally': 34,\n",
       " 'able': 12,\n",
       " 'thank': 38,\n",
       " 'donnie': 2,\n",
       " 'when': 364,\n",
       " 'another': 100,\n",
       " 'truckcrash': 1,\n",
       " 'overturns': 1,\n",
       " 'fortworth': 1,\n",
       " 'interstate': 2,\n",
       " 'rs22lj4qfp': 1,\n",
       " 'click': 9,\n",
       " 'been': 248,\n",
       " 'gt': 120,\n",
       " 'ld0uniyw4k': 1,\n",
       " 'ashville': 1,\n",
       " '23': 12,\n",
       " 'sb': 6,\n",
       " 'sr': 4,\n",
       " '752': 1,\n",
       " 'hylmo0wgfi': 1,\n",
       " 'construction': 6,\n",
       " 'guy': 34,\n",
       " 'working': 19,\n",
       " 'disney': 21,\n",
       " 'store': 11,\n",
       " 'gauges': 1,\n",
       " 'his': 190,\n",
       " 'ears': 9,\n",
       " 'bloody': 64,\n",
       " 'waiting': 19,\n",
       " 'robynjilllian': 1,\n",
       " 'wlsdomteeths': 1,\n",
       " 'going': 150,\n",
       " 'teesha': 1,\n",
       " 'come': 78,\n",
       " 'm42': 1,\n",
       " 'northbound': 2,\n",
       " 'junctions': 1,\n",
       " 'j3': 1,\n",
       " 'j3a': 1,\n",
       " 'currently': 17,\n",
       " 'delays': 8,\n",
       " '10': 75,\n",
       " 'mins': 8,\n",
       " 'c': 41,\n",
       " 'lwi3prba31': 1,\n",
       " 'daveoshry': 1,\n",
       " 'soembie': 1,\n",
       " 'say': 119,\n",
       " 'met': 5,\n",
       " 'super': 26,\n",
       " 'jelly': 1,\n",
       " 'dave': 2,\n",
       " 'p': 74,\n",
       " 'carolina': 2,\n",
       " 'motorcyclist': 12,\n",
       " '540': 2,\n",
       " 'crossed': 6,\n",
       " 'median': 1,\n",
       " 'motorcycle': 6,\n",
       " 'rider': 5,\n",
       " 'traveling': 2,\n",
       " 'p18lzrlmy6': 1,\n",
       " 'hit': 66,\n",
       " '500': 10,\n",
       " 'block': 19,\n",
       " 'se': 11,\n",
       " 'vista': 2,\n",
       " 'ter': 1,\n",
       " 'gresham': 5,\n",
       " 'pg15000044357': 1,\n",
       " '35': 13,\n",
       " 'pdx911': 5,\n",
       " 'fyi': 6,\n",
       " 'cad': 3,\n",
       " 'property': 32,\n",
       " 'nhs': 4,\n",
       " '999': 1,\n",
       " 'piner': 2,\n",
       " 'horndale': 2,\n",
       " 'dr': 16,\n",
       " 'naayf': 1,\n",
       " 'turning': 5,\n",
       " 'onto': 11,\n",
       " 'chandanee': 1,\n",
       " 'magu': 1,\n",
       " 'mma': 7,\n",
       " 'taxi': 3,\n",
       " 'rammed': 2,\n",
       " 'halfway': 2,\n",
       " 'turned': 18,\n",
       " 'confâ': 1,\n",
       " 'left': 42,\n",
       " 'manchester': 11,\n",
       " '293': 1,\n",
       " 'eddy': 1,\n",
       " 'stop': 83,\n",
       " 'nh': 3,\n",
       " '3a': 1,\n",
       " 'delay': 6,\n",
       " '4': 128,\n",
       " 'oia5fxi4gm': 1,\n",
       " 'wpd': 1,\n",
       " '1600': 2,\n",
       " '17th': 1,\n",
       " '09': 9,\n",
       " 'injury': 59,\n",
       " '2781': 1,\n",
       " 'willis': 1,\n",
       " 'foreman': 1,\n",
       " 'vckit6edev': 1,\n",
       " 'aashiqui': 1,\n",
       " 'actress': 2,\n",
       " 'anu': 1,\n",
       " 'aggarwal': 1,\n",
       " 'fatal': 92,\n",
       " '6otfp31lqw': 1,\n",
       " 'suffield': 1,\n",
       " 'alberta': 8,\n",
       " 'bptmlf4p10': 1,\n",
       " '9': 84,\n",
       " 'backup': 2,\n",
       " 'blocking': 6,\n",
       " 'right': 106,\n",
       " 'lanes': 8,\n",
       " 'exit': 12,\n",
       " 'langtree': 1,\n",
       " 'consider': 10,\n",
       " 'nc': 14,\n",
       " '115': 2,\n",
       " '150': 10,\n",
       " 'alternate': 2,\n",
       " 'changed': 5,\n",
       " 'determine': 3,\n",
       " 'options': 6,\n",
       " 'financially': 1,\n",
       " 'support': 37,\n",
       " 'plans': 37,\n",
       " 'treatment': 8,\n",
       " 'deadly': 12,\n",
       " 'hagerstown': 1,\n",
       " 'today': 147,\n",
       " 'll': 108,\n",
       " 'details': 13,\n",
       " '5': 138,\n",
       " 'your4state': 1,\n",
       " 'whag': 1,\n",
       " 'calum5sos': 4,\n",
       " 'qhmxuljsx9': 1,\n",
       " 'flowri': 1,\n",
       " 'were': 182,\n",
       " 'marinading': 1,\n",
       " 'donate': 7,\n",
       " 'spread': 3,\n",
       " 'word': 16,\n",
       " 'vaulter': 1,\n",
       " 'kira': 1,\n",
       " 'grã': 1,\n",
       " '_nberg': 1,\n",
       " 'paraplegic': 1,\n",
       " '6mpnycl8pk': 1,\n",
       " 'only': 127,\n",
       " 'even': 112,\n",
       " 'mfs': 4,\n",
       " 'drive': 24,\n",
       " 'norwaymfa': 1,\n",
       " 'bahrain': 1,\n",
       " 'previously': 4,\n",
       " 'road': 48,\n",
       " 'killed': 129,\n",
       " 'explosion': 58,\n",
       " 'gfjfgtodad': 1,\n",
       " 'leaders': 5,\n",
       " 'kenya': 5,\n",
       " 'forward': 7,\n",
       " 'comment': 19,\n",
       " 'issue': 14,\n",
       " 'disciplinary': 1,\n",
       " 'measures': 1,\n",
       " 'arrestpastornganga': 2,\n",
       " 'aftershock_delo': 2,\n",
       " 'scuf': 2,\n",
       " 'ps': 6,\n",
       " 'cya': 1,\n",
       " 'share': 23,\n",
       " 'page': 9,\n",
       " 'indoor': 1,\n",
       " 'trampoline': 1,\n",
       " 'park': 38,\n",
       " 'aftershock': 33,\n",
       " 'opening': 13,\n",
       " 'fall': 62,\n",
       " 'ugxhherrxs': 1,\n",
       " 'bxckylynch': 1,\n",
       " 'foi': 1,\n",
       " 'roh': 1,\n",
       " 'las': 5,\n",
       " 'vegas': 8,\n",
       " 'procura': 1,\n",
       " 'pirate': 2,\n",
       " 'bay': 16,\n",
       " 'que': 4,\n",
       " 'tem': 1,\n",
       " 'himself': 14,\n",
       " 'further': 5,\n",
       " 'once': 27,\n",
       " 'effort': 7,\n",
       " 'gets': 45,\n",
       " 'painful': 2,\n",
       " 'win': 25,\n",
       " 'roger': 2,\n",
       " 'bannister': 1,\n",
       " 'schoolboy': 1,\n",
       " 'original': 12,\n",
       " 'mix': 15,\n",
       " 'excision': 1,\n",
       " 'skism': 1,\n",
       " 'sexism': 1,\n",
       " 'too': 107,\n",
       " 'loud': 62,\n",
       " 'remix': 4,\n",
       " 'firebeatz': 1,\n",
       " 'schella': 1,\n",
       " 'dear': 10,\n",
       " 'jqlzua6yzq': 1,\n",
       " '320': 10,\n",
       " 'ir': 11,\n",
       " 'icemoon': 10,\n",
       " 'ynxnvvkcda': 1,\n",
       " 'djicemoon': 10,\n",
       " 'dubstep': 11,\n",
       " 'trapmusic': 10,\n",
       " 'dnb': 11,\n",
       " 'dance': 18,\n",
       " 'icesâ': 10,\n",
       " 'weqpesenku': 1,\n",
       " 'thyzomvwu0': 4,\n",
       " '83joo0xk29': 4,\n",
       " 'victory': 8,\n",
       " 'bargain': 8,\n",
       " 'basement': 6,\n",
       " 'prices': 6,\n",
       " 'dwight': 2,\n",
       " 'david': 13,\n",
       " 'eisenhower': 2,\n",
       " 'nepal': 11,\n",
       " 'int': 3,\n",
       " 'l': 18,\n",
       " ...}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. 텍스트에서 단어를 추출 : extract_words(sentence) \n",
    "def extract_words(sentence):\n",
    "    '''This is to clean and tokenize words'''\n",
    "    # 특수 문자를 공백으로 바꿉니다.\n",
    "    ignore_words = ['a', 'the', 'if', 'br', 'and', 'of', 'to', 'is', 'are', 'he', 'she', 'my', 'you', 'it','how']\n",
    "    words = re.sub(\"[^\\w]\", \" \",  sentence).split() # 모든 특수 문자를 ' '로 대체합니다.\n",
    "    words = [word.lower() for word in words]   \n",
    "    words_cleaned = [w.lower() for w in words if w not in ignore_words]   \n",
    "    return words_cleaned \n",
    "    \n",
    "# 2. 단어의 빈도를 계산합니다. \n",
    "def map_book(hash_map, tokens):\n",
    "    if tokens is not None:\n",
    "        for word in tokens:\n",
    "            # 단어가 존재합니까?\n",
    "            if word in hash_map:\n",
    "                hash_map[word] = hash_map[word] + 1\n",
    "            else:\n",
    "                hash_map[word] = 1\n",
    "        return hash_map\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# 3.각 단어의 발생 빈도를 계산한 값으로 hash map을 만듭니다. = > 2 단계 호출\n",
    "def make_hash_map(df):\n",
    "    # 해시맵을 생성합니다.\n",
    "    hash_map = {}\n",
    "    \n",
    "    # DataFrame의 각 행에 대해 반복합니다.\n",
    "    for index, row in df.iterrows():\n",
    "        # 단어의 빈도를 계산합니다.\n",
    "        hash_map = map_book(hash_map, extract_words(row['text']))\n",
    "    \n",
    "    # 해시맵을 반환합니다.\n",
    "    return hash_map\n",
    "\n",
    "#토큰 화 된 데이터 세트에서 해시 맵 (단어 및 빈도) 생성\n",
    "hash_map = make_hash_map(df)   \n",
    "hash_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['t', 'co', 'http', 'in', 'i', 'ã', 's', 'for', 'on', 'that']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4.frequent_vocab 함수를 다음과 같은 입력으로 정의하십시오 : word_freq 및 max_features\n",
    "def frequent_vocab(word_freq, max_features): \n",
    "    counter = 0  # 값 0으로 카운터를 초기화하십시오\n",
    "    vocab = []   # Vocab이라는 빈 목록을 만듭니다\n",
    "    # 단어를 빈도수가 낮은 순서로 사전에 나열합니다\n",
    "    for key, value in sorted(word_freq.items(), key=lambda item: (item[1], item[0]), reverse=True): \n",
    "       # 상위(max_features) 단어 수를 얻기 위한 루프 함수\n",
    "        if counter<max_features: \n",
    "            vocab.append(key)\n",
    "            counter+=1\n",
    "        else: break\n",
    "    return vocab\n",
    "\n",
    "# 빈도수 높은 상위 500개 단어 선정     \n",
    "vocab = frequent_vocab(hash_map, 500)  \n",
    "vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5.다음과 같은 입력으로 함수 bagofwords를 정의: sentence, words\n",
    "def bagofwords(sentence, words):\n",
    "    sentence_words = extract_words(sentence) # 문장/트윗을 토큰화하고 변수 sentence_words에 할당\n",
    "    # 빈도 단어 수\n",
    "    bag = np.zeros(len(words)) # 크기가 len(words)이고 0으로 구성된 NumPy 배열 생성\n",
    "    # 트윗에 토큰이 있을 때 데이터를 반복하고 1의 값을 추가\n",
    "    for sw in sentence_words:\n",
    "        for i,word in enumerate(words):\n",
    "            if word == sw: \n",
    "                bag[i] += 1\n",
    "                \n",
    "    return np.array(bag) # 하나의 트윗에 대한 단어 가방 반환\n",
    "\n",
    "# 6. bow 생성 : df['text'] 에 대해 단어 가방을 포함하도록 지정된 차원이있는 숫자 배열을 설정합니다.\n",
    "n_words = len(vocab)\n",
    "n_docs = len(df)\n",
    "bag_o = np.zeros([n_docs,n_words])\n",
    "# 루프 함수를 사용하여 각 트윗에 대해 새 행을 추가합니다.\n",
    "for ii in range(n_docs): \n",
    "    # 이전 함수 'bagofwords'를 호출합니다. 입력을 참조하십시오 : sentence, words\n",
    "    bag_o[ii,:] = bagofwords(df['text'].iloc[ii], vocab) \n",
    "\n",
    "bag_o[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.56037575, 0.64356806, 0.74842242, 1.45884525, 1.73955499])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7.idf 구하기\n",
    "# 트윗 수(numdocs)와 토큰/워드 수(numwords)를 나타내는 변수 2개 초기화\n",
    "numdocs, numwords = np.shape(bag_o)\n",
    "# 위와 같이 TFIDF 수식으로 변경\n",
    "N = numdocs\n",
    "word_frequency = np.empty(numwords)\n",
    "# 단어가 나타나는 문서 수를 계산\n",
    "for word in range(numwords):\n",
    "    word_frequency[word]=np.sum((bag_o[:,word]>0)) \n",
    "\n",
    "idf = np.log(N/word_frequency)\n",
    "idf[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8.tf-idf 구하기\n",
    "# 초기화 tfidf 배열\n",
    "tfidf = np.empty([numdocs, numwords])\n",
    "\n",
    "# 트윗에서 반복, 용어 빈도(단어 가방으로 표시)를 idf로 곱합니다.\n",
    "for doc in range(numdocs):\n",
    "    tfidf[doc, :]=bag_o[doc, :]*idf\n",
    "\n",
    "tfidf[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 학습용/테스트용 데이터셋 분리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_all과 y_all을 교육 및 테스트 세트로 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split( tfidf , df['relevance'].values, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### < 기계학습하기 >\n",
    "#### 1. 라이브러리 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression #로지스틱 회귀 모형 가져오기\n",
    "from sklearn.model_selection import train_test_split # 데이터를 훈련 및 테스트 세트로 분할\n",
    "from sklearn.model_selection import GridSearchCV # 모델의 가장 적합한 매개 변수를 찾기 위해"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 모델 생성하고 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 인스턴스(instance) 작성\n",
    "logreg = LogisticRegression(solver = 'liblinear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 데이터에 대한 모델 훈련, 데이터로부터 배운 정보 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(solver=&#x27;liblinear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(solver='liblinear')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습 세트에 모델 적합\n",
    "logreg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 모델을 사용하여 테스트 데이터를 기반으로 관련성 예측 및 정확도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 ... 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "y_pred = logreg.predict(X_test)\n",
    "print (y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7686924493554328\n",
      "Accuracy of logistic regression classifier on test set: 0.769\n"
     ]
    }
   ],
   "source": [
    "# 스코어 방법을 사용하여 모델의 정확성을 얻습니다\n",
    "score = logreg.score(X_test, y_test)\n",
    "print(score)\n",
    "print('Accuracy of logistic regression classifier on test set: {:.3f}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 재난 예측기로 테스트 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twitter_predictor(tweet):\n",
    "    # your code here - 코드 완성됨\n",
    "    word_vector = bagofwords(tweet, vocab) # 단어 가방 변수를 설정합니다.bagofwords 함수를 기억하십니까?\n",
    "    word_tfidf = word_vector*idf # tfidf값 찾기\n",
    "    prediction = logreg.predict(word_tfidf.reshape(1, -1)) # 트윗이 자연재해와 관련이 있는지 없는지 예측\n",
    "    results = {1:'Relevant', 0:'Not Relevant'} # 잠재적인 결과를 포함하는 집합을 만듭니다.\"Relevant\" 및 \"Not relevant\" 태그를 변경할 수 있습니다.\n",
    "    #print(results[int(prediction)])  : prediction은 1D 배열이며, 이를 직접 int() 함수에 전달하려고 하면 경고\n",
    "    print(results[int(prediction[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant\n",
      "Not Relevant\n",
      "Not Relevant\n",
      "Relevant\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "tweet1 = '200 houses were on fire after an electric spark burns the stack of wood'\n",
    "tweet2 = 'Michael curry is on the roll as he scored the fifth goal on the football tournament.'\n",
    "tweet3 = 'Michael curry is on fire as he scored the fifth goal on the football tournament.'\n",
    "tweet4 = \"Natural disasters can cause significant damage to homes, infrastructure, and the environment, leading to loss of life and displacement of communities.\"\n",
    "\n",
    "twitter_predictor(tweet1)\n",
    "twitter_predictor(tweet2)\n",
    "twitter_predictor(tweet3)\n",
    "twitter_predictor(tweet4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 방법2 : 라이브러리 사용하여 재난예측기 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8084714548802947\n",
      "[0 0 0 ... 0 0 1]\n",
      "Accuracy : 80.85 %\n",
      " \"200 houses were on fire after an electric spark burns the stack of wood\" is predicted as : Relevant\n",
      " \"Michael curry is on the roll as he scored the fifth goal on the football tournament.\" is predicted as : Not Relevant\n",
      " \"Michael curry is on fire as he scored the fifth goal on the football tournament.\" is predicted as : Not Relevant\n",
      " \"Natural disasters can cause significant damage to homes, infrastructure, and the environment, leading to loss of life and displacement of communities.\" is predicted as : Relevant\n"
     ]
    }
   ],
   "source": [
    "# khs code\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "df_raw = pd.read_csv('[Dataset]_Module25_disasters_social_media.csv',encoding='ISO-8859-1')\n",
    "\n",
    "df_choice_one = df_raw[df_raw['choose_one'] != \"Can't Decide\"]\n",
    "df = df_choice_one[['text', 'choose_one']].copy()\n",
    "df['relevance'] = df['choose_one'].replace({'Relevant' : 1, 'Not Relevant' : 0})\n",
    "# relevance = {'Relevant':1,'Not Relevant':0}\n",
    "# df['relevance'] = df.choose_one.map(relevance) \n",
    "\n",
    "X_all = df[\"text\"]\n",
    "y_all = df['relevance'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( X_all , y_all, shuffle=True)\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = \"word\", strip_accents=None, tokenizer = None, \\\n",
    "                             preprocessor = None, stop_words = None, max_features = 5000) \n",
    "train_data_features = vectorizer.fit_transform(X_train)\n",
    "test_data_features = vectorizer.transform(X_test)\n",
    "\n",
    "tfidfier = TfidfTransformer()\n",
    "tfidf = tfidfier.fit_transform(train_data_features)\n",
    "tfidf_test = tfidfier.transform(test_data_features)\n",
    "\n",
    "tfidf_X_train = tfidf.toarray()\n",
    "tfidf_X_test = tfidf_test.toarray()\n",
    "\n",
    "def classify():\n",
    "    rf = LogisticRegression()\n",
    "    rf.fit(tfidf_X_train, y_train)\n",
    "    print(rf.score(tfidf_X_test, y_test))\n",
    "    print(rf.predict(tfidf_X_test))\n",
    "    return rf\n",
    "\n",
    "model = classify()\n",
    "y_pred = model.predict(tfidf_X_test)\n",
    "print(f\"Accuracy : {((model.score(tfidf_X_test,y_test))*100).round(2)} %\")\n",
    "\n",
    "def twitter_predictor(model, input_sentence):\n",
    "    # CountVectorizer와 TfidfTransformer를 이용하여 입력 문장 변환\n",
    "    input_data_features = vectorizer.transform([input_sentence])\n",
    "    input_tfidf = tfidfier.transform(input_data_features)\n",
    "\n",
    "    # rf = classify()\n",
    "    # 변환된 입력을 모델에 적용하여 감정 예측\n",
    "    prediction = model.predict(input_tfidf)\n",
    "    results = {1: 'Relevant', 0: 'Not Relevant'} \n",
    "\n",
    "    # 결과 출력\n",
    "    print(f' \"{input_sentence}\" is predicted as : {results[prediction[0]]}' )\n",
    "\n",
    "test_sentence =[\n",
    "                \"200 houses were on fire after an electric spark burns the stack of wood\",\n",
    "                \"Michael curry is on the roll as he scored the fifth goal on the football tournament.\",\n",
    "                \"Michael curry is on fire as he scored the fifth goal on the football tournament.\",\n",
    "                \"Natural disasters can cause significant damage to homes, infrastructure, and the environment, leading to loss of life and displacement of communities.\"]\n",
    "\n",
    "for test in test_sentence : \n",
    "    twitter_predictor(model, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [감정분석 예측기 - 영화 데이터 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 방법1 : 라이브러리 사용하여 예측기 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 라이브러리가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer # 이 기능은 단어 가방을 만들 수 있도록 도와줍니다.\n",
    "from sklearn.feature_extraction.text import TfidfTransformer # 이 기능은 단어 가방을 자동으로 정규화합니다.\n",
    "from sklearn.linear_model import LogisticRegression #로지스틱 회귀 모형 가져오기\n",
    "from sklearn.model_selection import train_test_split # 데이터를 훈련 및 테스트 세트로 분할\n",
    "from sklearn.model_selection import GridSearchCV # 모델의 가장 적합한 매개 변수를 찾기 위해"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 데이터 읽어 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_raw = pd.read_pickle('[Dataset]_Module25_df_raw.pkl')\n",
    "df_raw_test = pd.read_pickle('[Dataset]_Module25_df_raw_test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.BOW 생성하기 - CountVerorize()사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer = \"word\", strip_accents=None, tokenizer = None, \\\n",
    "                             preprocessor = None, stop_words = None, max_features = 5000) \n",
    "train_data_features = vectorizer.fit_transform(df_raw['text'])\n",
    "test_data_features = vectorizer.transform(df_raw_test['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. BOW 정규화하기 - TfidfTransformer() => .fit_transform() 메서드를 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfidfier = TfidfTransformer()\n",
    "tfidf = tfidfier.fit_transform(train_data_features)\n",
    "tfidf_test = tfidfier.transform(test_data_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 학습/테스트 데이터셋 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_all = tfidf.toarray()\n",
    "y_all = df_raw['positive'].values\n",
    "X_test = tfidf_test.toarray()\n",
    "y_test = df_raw_test['positive'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 분류기 만들어 예측하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.88252\n",
      "[1 1 1 ... 1 1 1]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def classify():\n",
    "    rf = LogisticRegression()\n",
    "    rf.fit(X_all,y_all)\n",
    "    print(rf.score(X_test,y_test))\n",
    "    print(rf.predict(X_test))\n",
    "    return rf\n",
    "\n",
    "classify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.88252\n",
      "[1 1 1 ... 1 1 1]\n",
      "The sentence is predicted as: Positive\n"
     ]
    }
   ],
   "source": [
    "def sentiment_prediction(input_sentence):\n",
    "    # CountVectorizer와 TfidfTransformer를 이용하여 입력 문장 변환\n",
    "    input_data_features = vectorizer.transform([input_sentence])\n",
    "    input_tfidf = tfidfier.transform(input_data_features)\n",
    "\n",
    "    rf = classify()\n",
    "    # 변환된 입력을 모델에 적용하여 감정 예측\n",
    "    prediction = rf.predict(input_tfidf)\n",
    "    results = {1: 'Positive', 0: 'Negative'} \n",
    "\n",
    "    # 결과 출력\n",
    "    print('The sentence is predicted as:', results[prediction[0]])\n",
    "\n",
    "test_sentence = \"The movie was absolutely fantastic! The storyline was captivating, the acting was superb, and the visuals were stunning.\"\n",
    "sentiment_prediction(test_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### 방법2 : 파이썬 프로그래밍으로 예측기 만들기 ( tfidf 구하기 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer # 이 기능은 단어 가방을 만들 수 있도록 도와줍니다.\n",
    "from sklearn.feature_extraction.text import TfidfTransformer # 이 기능은 단어 가방을 자동으로 정규화합니다.\n",
    "from sklearn.linear_model import LogisticRegression #로지스틱 회귀 모형 가져오기\n",
    "from sklearn.model_selection import train_test_split # 데이터를 훈련 및 테스트 세트로 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_raw = pd.read_pickle('[Dataset]_Module25_df_raw.pkl')\n",
    "df_raw_test = pd.read_pickle('[Dataset]_Module25_df_raw_test.pkl')\n",
    "df_all = pd.concat([df_raw, df_raw_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df_all[['text','positive']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract_words(sentence):\n",
    "    '''This is to clean and tokenize words'''\n",
    "    # 특수 문자를 공백으로 바꿉니다.\n",
    "    ignore_words = ['a', 'the', 'if', 'br', 'and', 'of', 'to', 'is', 'are', 'he', 'she', 'my', 'you', 'it','how']\n",
    "    words = re.sub(\"[^\\w]\", \" \",  sentence).split() # 모든 특수 문자를 ' '로 대체합니다.\n",
    "    words = [word.lower() for word in words]\n",
    "    words_cleaned = [w.lower() for w in words if w not in ignore_words]\n",
    "    return words_cleaned "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bromwell': 8,\n",
       " 'high': 4342,\n",
       " 'cartoon': 1099,\n",
       " 'comedy': 6576,\n",
       " 'ran': 482,\n",
       " 'at': 46800,\n",
       " 'same': 8096,\n",
       " 'time': 25110,\n",
       " 'as': 91750,\n",
       " 'some': 31134,\n",
       " 'other': 18274,\n",
       " 'programs': 117,\n",
       " 'about': 34160,\n",
       " 'school': 3535,\n",
       " 'life': 12917,\n",
       " 'such': 10005,\n",
       " 'teachers': 159,\n",
       " '35': 184,\n",
       " 'years': 8759,\n",
       " 'in': 186781,\n",
       " 'teaching': 201,\n",
       " 'profession': 133,\n",
       " 'lead': 2696,\n",
       " 'me': 21457,\n",
       " 'believe': 4991,\n",
       " 'that': 143879,\n",
       " 's': 125008,\n",
       " 'satire': 495,\n",
       " 'much': 19318,\n",
       " 'closer': 379,\n",
       " 'reality': 2010,\n",
       " 'than': 19330,\n",
       " 'scramble': 10,\n",
       " 'survive': 546,\n",
       " 'financially': 58,\n",
       " 'insightful': 141,\n",
       " 'students': 749,\n",
       " 'who': 42234,\n",
       " 'can': 29059,\n",
       " 'see': 23029,\n",
       " 'right': 6529,\n",
       " 'through': 9692,\n",
       " 'their': 22750,\n",
       " 'pathetic': 987,\n",
       " 'pomp': 14,\n",
       " 'pettiness': 6,\n",
       " 'whole': 6122,\n",
       " 'situation': 1384,\n",
       " 'all': 46947,\n",
       " 'remind': 285,\n",
       " 'schools': 140,\n",
       " 'i': 175633,\n",
       " 'knew': 1826,\n",
       " 'when': 28062,\n",
       " 'saw': 6341,\n",
       " 'episode': 3183,\n",
       " 'which': 23402,\n",
       " 'student': 807,\n",
       " 'repeatedly': 267,\n",
       " 'tried': 1589,\n",
       " 'burn': 253,\n",
       " 'down': 7431,\n",
       " 'immediately': 925,\n",
       " 'recalled': 40,\n",
       " 'classic': 3580,\n",
       " 'line': 3683,\n",
       " 'inspector': 260,\n",
       " 'm': 10000,\n",
       " 'here': 11109,\n",
       " 'sack': 78,\n",
       " 'one': 53603,\n",
       " 'your': 11497,\n",
       " 'welcome': 382,\n",
       " 'expect': 2356,\n",
       " 'many': 13446,\n",
       " 'adults': 700,\n",
       " 'age': 2064,\n",
       " 'think': 14337,\n",
       " 'far': 5994,\n",
       " 'fetched': 215,\n",
       " 'what': 32239,\n",
       " 'pity': 447,\n",
       " 'isn': 6318,\n",
       " 't': 68277,\n",
       " 'homelessness': 9,\n",
       " 'or': 35779,\n",
       " 'houselessness': 1,\n",
       " 'george': 1674,\n",
       " 'carlin': 38,\n",
       " 'stated': 251,\n",
       " 'has': 33038,\n",
       " 'been': 18350,\n",
       " 'an': 42986,\n",
       " 'issue': 589,\n",
       " 'for': 87471,\n",
       " 'but': 83554,\n",
       " 'never': 12981,\n",
       " 'plan': 834,\n",
       " 'help': 3689,\n",
       " 'those': 9392,\n",
       " 'on': 68057,\n",
       " 'street': 1318,\n",
       " 'were': 21209,\n",
       " 'once': 4615,\n",
       " 'considered': 919,\n",
       " 'human': 3220,\n",
       " 'did': 12624,\n",
       " 'everything': 4829,\n",
       " 'from': 40498,\n",
       " 'going': 8190,\n",
       " 'work': 8546,\n",
       " 'vote': 498,\n",
       " 'matter': 2447,\n",
       " 'most': 17378,\n",
       " 'people': 18188,\n",
       " 'homeless': 194,\n",
       " 'just': 35184,\n",
       " 'lost': 2948,\n",
       " 'cause': 1082,\n",
       " 'while': 10363,\n",
       " 'worrying': 73,\n",
       " 'things': 7335,\n",
       " 'racism': 344,\n",
       " 'war': 4448,\n",
       " 'iraq': 149,\n",
       " 'pressuring': 10,\n",
       " 'kids': 3663,\n",
       " 'succeed': 305,\n",
       " 'technology': 435,\n",
       " 'elections': 18,\n",
       " 'inflation': 11,\n",
       " 'they': 45383,\n",
       " 'll': 5795,\n",
       " 'be': 53383,\n",
       " 'next': 3505,\n",
       " 'end': 11121,\n",
       " 'up': 26394,\n",
       " 'streets': 511,\n",
       " 'given': 3571,\n",
       " 'bet': 426,\n",
       " 'live': 2994,\n",
       " 'month': 276,\n",
       " 'without': 6438,\n",
       " 'luxuries': 8,\n",
       " 'had': 22064,\n",
       " 'home': 3705,\n",
       " 'entertainment': 1686,\n",
       " 'sets': 1656,\n",
       " 'bathroom': 225,\n",
       " 'pictures': 849,\n",
       " 'wall': 708,\n",
       " 'computer': 865,\n",
       " 'treasure': 358,\n",
       " 'like': 40172,\n",
       " 'goddard': 24,\n",
       " 'bolt': 37,\n",
       " 'lesson': 494,\n",
       " 'mel': 267,\n",
       " 'brooks': 371,\n",
       " 'directs': 187,\n",
       " 'stars': 3137,\n",
       " 'plays': 4332,\n",
       " 'rich': 1203,\n",
       " 'man': 11823,\n",
       " 'world': 7496,\n",
       " 'until': 3504,\n",
       " 'deciding': 101,\n",
       " 'make': 15899,\n",
       " 'with': 87368,\n",
       " 'sissy': 104,\n",
       " 'rival': 303,\n",
       " 'jeffery': 31,\n",
       " 'tambor': 13,\n",
       " 'thirty': 298,\n",
       " 'days': 2591,\n",
       " 'succeeds': 340,\n",
       " 'do': 18224,\n",
       " 'wants': 2530,\n",
       " 'future': 1672,\n",
       " 'project': 1007,\n",
       " 'making': 5779,\n",
       " 'more': 28016,\n",
       " 'buildings': 204,\n",
       " 'where': 12693,\n",
       " 'thrown': 828,\n",
       " 'bracelet': 16,\n",
       " 'his': 57556,\n",
       " 'leg': 211,\n",
       " 'monitor': 48,\n",
       " 'every': 7967,\n",
       " 'move': 1464,\n",
       " 'step': 697,\n",
       " 'off': 12050,\n",
       " 'sidewalk': 83,\n",
       " 'nickname': 47,\n",
       " 'pepto': 5,\n",
       " 'by': 44482,\n",
       " 'vagrant': 6,\n",
       " 'after': 14984,\n",
       " 'written': 3111,\n",
       " 'forehead': 63,\n",
       " 'meets': 1309,\n",
       " 'characters': 14456,\n",
       " 'including': 2086,\n",
       " 'woman': 5341,\n",
       " 'name': 3275,\n",
       " 'molly': 159,\n",
       " 'lesley': 33,\n",
       " 'ann': 501,\n",
       " 'warren': 188,\n",
       " 'ex': 926,\n",
       " 'dancer': 280,\n",
       " 'got': 6949,\n",
       " 'divorce': 199,\n",
       " 'before': 8523,\n",
       " 'losing': 430,\n",
       " 'her': 34803,\n",
       " 'pals': 76,\n",
       " 'sailor': 126,\n",
       " 'howard': 590,\n",
       " 'morris': 186,\n",
       " 'fumes': 8,\n",
       " 'teddy': 90,\n",
       " 'wilson': 425,\n",
       " 'already': 2675,\n",
       " 'used': 3871,\n",
       " 're': 8904,\n",
       " 'survivors': 193,\n",
       " 'not': 60748,\n",
       " 'reaching': 169,\n",
       " 'mutual': 105,\n",
       " 'agreements': 7,\n",
       " 'being': 13123,\n",
       " 'fight': 2212,\n",
       " 'flight': 381,\n",
       " 'kill': 2469,\n",
       " 'killed': 2309,\n",
       " 'love': 13008,\n",
       " 'connection': 544,\n",
       " 'between': 6580,\n",
       " 'wasn': 4489,\n",
       " 'necessary': 614,\n",
       " 'plot': 12987,\n",
       " 'found': 5161,\n",
       " 'stinks': 182,\n",
       " 'observant': 33,\n",
       " 'films': 13755,\n",
       " 'prior': 382,\n",
       " 'shows': 4705,\n",
       " 'tender': 197,\n",
       " 'side': 2554,\n",
       " 'compared': 1029,\n",
       " 'slapstick': 424,\n",
       " 'blazing': 81,\n",
       " 'saddles': 42,\n",
       " 'young': 7045,\n",
       " 'frankenstein': 268,\n",
       " 'spaceballs': 23,\n",
       " 'show': 12657,\n",
       " 'having': 4931,\n",
       " 'something': 10143,\n",
       " 'valuable': 174,\n",
       " 'day': 5256,\n",
       " 'hand': 2381,\n",
       " 'stupid': 3532,\n",
       " 'don': 17623,\n",
       " 'know': 12510,\n",
       " 'money': 4515,\n",
       " 'maybe': 4740,\n",
       " 'should': 9772,\n",
       " 'give': 6702,\n",
       " 'instead': 4316,\n",
       " 'using': 1539,\n",
       " 'monopoly': 16,\n",
       " 'this': 151002,\n",
       " 'film': 79705,\n",
       " 'will': 18115,\n",
       " 'inspire': 139,\n",
       " 'others': 3176,\n",
       " 'brilliant': 2414,\n",
       " 'over': 12215,\n",
       " 'acting': 12867,\n",
       " 'best': 12612,\n",
       " 'dramatic': 1233,\n",
       " 'hobo': 19,\n",
       " 'lady': 1557,\n",
       " 'have': 55203,\n",
       " 'ever': 12022,\n",
       " 'seen': 13375,\n",
       " 'scenes': 10482,\n",
       " 'clothes': 629,\n",
       " 'warehouse': 93,\n",
       " 'second': 3830,\n",
       " 'none': 1979,\n",
       " 'corn': 144,\n",
       " 'face': 3244,\n",
       " 'good': 29753,\n",
       " 'anything': 5765,\n",
       " 'take': 6987,\n",
       " 'lawyers': 70,\n",
       " 'also': 17977,\n",
       " 'superb': 1300,\n",
       " 'accused': 254,\n",
       " 'turncoat': 8,\n",
       " 'selling': 290,\n",
       " 'out': 34202,\n",
       " 'boss': 782,\n",
       " 'dishonest': 42,\n",
       " 'lawyer': 352,\n",
       " 'shrugs': 12,\n",
       " 'indifferently': 6,\n",
       " 'says': 2258,\n",
       " 'three': 4703,\n",
       " 'funny': 8758,\n",
       " 'words': 1832,\n",
       " 'jeffrey': 190,\n",
       " 'favorite': 2391,\n",
       " 'later': 4279,\n",
       " 'larry': 455,\n",
       " 'sanders': 116,\n",
       " 'fantastic': 1519,\n",
       " 'too': 15383,\n",
       " 'mad': 1027,\n",
       " 'millionaire': 133,\n",
       " 'crush': 174,\n",
       " 'ghetto': 107,\n",
       " 'character': 13905,\n",
       " 'malevolent': 48,\n",
       " 'usual': 1852,\n",
       " 'hospital': 773,\n",
       " 'scene': 10968,\n",
       " 'invade': 38,\n",
       " 'demolition': 40,\n",
       " 'site': 497,\n",
       " 'classics': 493,\n",
       " 'look': 8295,\n",
       " 'legs': 292,\n",
       " 'two': 13545,\n",
       " 'big': 6962,\n",
       " 'diggers': 23,\n",
       " 'fighting': 1176,\n",
       " 'bleeds': 21,\n",
       " 'movie': 87971,\n",
       " 'gets': 6236,\n",
       " 'better': 11430,\n",
       " 'each': 5211,\n",
       " 'quite': 7275,\n",
       " 'often': 3124,\n",
       " 'easily': 1653,\n",
       " 'underrated': 465,\n",
       " 'inn': 43,\n",
       " 'cannon': 174,\n",
       " 'sure': 5296,\n",
       " 'its': 16062,\n",
       " 'flawed': 320,\n",
       " 'does': 11627,\n",
       " 'realistic': 1516,\n",
       " 'view': 1938,\n",
       " 'unlike': 1137,\n",
       " 'say': 10770,\n",
       " 'citizen': 234,\n",
       " 'kane': 357,\n",
       " 'gave': 2407,\n",
       " 'lounge': 30,\n",
       " 'singers': 143,\n",
       " 'titanic': 240,\n",
       " 'italians': 86,\n",
       " 'idiots': 242,\n",
       " 'jokes': 1992,\n",
       " 'fall': 1555,\n",
       " 'flat': 1099,\n",
       " 'still': 10859,\n",
       " 'very': 27727,\n",
       " 'lovable': 301,\n",
       " 'way': 15645,\n",
       " 'comedies': 846,\n",
       " 'pull': 727,\n",
       " 'story': 23119,\n",
       " 'traditionally': 49,\n",
       " 'reviled': 16,\n",
       " 'members': 1122,\n",
       " 'society': 1375,\n",
       " 'truly': 3467,\n",
       " 'impressive': 956,\n",
       " 'fisher': 163,\n",
       " 'king': 1739,\n",
       " 'crap': 2080,\n",
       " 'either': 3672,\n",
       " 'only': 23241,\n",
       " 'complaint': 232,\n",
       " 'cast': 7426,\n",
       " 'someone': 4601,\n",
       " 'else': 3993,\n",
       " 'director': 8812,\n",
       " 'writer': 2235,\n",
       " 'so': 40911,\n",
       " 'typical': 1536,\n",
       " 'was': 95608,\n",
       " 'less': 3763,\n",
       " 'movies': 15309,\n",
       " 'actually': 8470,\n",
       " 'followable': 6,\n",
       " 'leslie': 292,\n",
       " 'made': 16152,\n",
       " 'under': 2704,\n",
       " 'rated': 951,\n",
       " 'actress': 2330,\n",
       " 'there': 37524,\n",
       " 'moments': 3270,\n",
       " 'could': 15565,\n",
       " 'fleshed': 91,\n",
       " 'bit': 5967,\n",
       " 'probably': 5622,\n",
       " 'cut': 2064,\n",
       " 'room': 1847,\n",
       " 'worth': 4674,\n",
       " 'price': 580,\n",
       " 'rent': 1497,\n",
       " 'overall': 2872,\n",
       " 'himself': 4188,\n",
       " 'job': 4463,\n",
       " 'characteristic': 81,\n",
       " 'speaking': 803,\n",
       " 'directly': 386,\n",
       " 'audience': 4317,\n",
       " 'again': 7884,\n",
       " 'actor': 4647,\n",
       " 'fume': 6,\n",
       " 'both': 6809,\n",
       " 'played': 5189,\n",
       " 'parts': 2400,\n",
       " 'well': 21268,\n",
       " 'comedic': 610,\n",
       " 'robin': 555,\n",
       " 'williams': 609,\n",
       " 'nor': 1283,\n",
       " 'quirky': 368,\n",
       " 'insane': 510,\n",
       " 'recent': 970,\n",
       " 'thriller': 1746,\n",
       " 'fame': 519,\n",
       " 'hybrid': 109,\n",
       " 'drama': 2830,\n",
       " 'dramatization': 44,\n",
       " 'mixed': 543,\n",
       " 'new': 8096,\n",
       " 'per': 277,\n",
       " 'se': 94,\n",
       " 'mystery': 1533,\n",
       " 'suspense': 1602,\n",
       " 'vehicle': 428,\n",
       " 'attempts': 1150,\n",
       " 'locate': 100,\n",
       " 'sick': 985,\n",
       " 'boy': 3158,\n",
       " 'keeper': 80,\n",
       " 'starring': 931,\n",
       " 'sandra': 280,\n",
       " 'oh': 2977,\n",
       " 'rory': 107,\n",
       " 'culkin': 42,\n",
       " 'pretty': 7254,\n",
       " 'news': 745,\n",
       " 'report': 201,\n",
       " 'william': 1175,\n",
       " 'close': 2470,\n",
       " 'achieving': 66,\n",
       " 'goal': 248,\n",
       " 'must': 6544,\n",
       " 'highly': 2278,\n",
       " 'entertained': 432,\n",
       " 'though': 8752,\n",
       " 'fails': 1170,\n",
       " 'teach': 280,\n",
       " 'guide': 300,\n",
       " 'inspect': 13,\n",
       " 'amuse': 58,\n",
       " 'felt': 2875,\n",
       " 'watching': 9165,\n",
       " 'guy': 6390,\n",
       " 'performing': 258,\n",
       " 'actions': 657,\n",
       " 'third': 1381,\n",
       " 'person': 3236,\n",
       " 'perspective': 480,\n",
       " 'real': 9430,\n",
       " 'able': 2624,\n",
       " 'subscribe': 18,\n",
       " 'premise': 1409,\n",
       " 'watch': 13947,\n",
       " 'definitely': 3110,\n",
       " 'friday': 410,\n",
       " 'saturday': 428,\n",
       " 'night': 4231,\n",
       " 'fare': 370,\n",
       " 'rates': 155,\n",
       " '7': 1747,\n",
       " '10': 8614,\n",
       " 'fiend': 76,\n",
       " 'yes': 3001,\n",
       " 'art': 2492,\n",
       " 'successfully': 286,\n",
       " 'slow': 2187,\n",
       " 'paced': 587,\n",
       " 'unfolds': 205,\n",
       " 'nice': 3863,\n",
       " 'volumes': 69,\n",
       " 'even': 24871,\n",
       " 'notice': 778,\n",
       " 'happening': 743,\n",
       " 'fine': 2545,\n",
       " 'performance': 5575,\n",
       " 'sexuality': 306,\n",
       " 'angles': 417,\n",
       " 'seem': 4230,\n",
       " 'unnecessary': 570,\n",
       " 'affect': 167,\n",
       " 'enjoy': 3583,\n",
       " 'however': 6958,\n",
       " 'core': 575,\n",
       " 'engaging': 574,\n",
       " 'doesn': 8876,\n",
       " 'rush': 288,\n",
       " 'onto': 671,\n",
       " 'grips': 64,\n",
       " 'enough': 6895,\n",
       " 'keep': 3236,\n",
       " 'wondering': 698,\n",
       " 'direction': 2726,\n",
       " 'use': 3577,\n",
       " 'lights': 378,\n",
       " 'achieve': 335,\n",
       " 'desired': 183,\n",
       " 'affects': 112,\n",
       " 'unexpectedness': 2,\n",
       " '1': 4308,\n",
       " 'looking': 5053,\n",
       " 'lay': 203,\n",
       " 'back': 9677,\n",
       " 'hear': 1429,\n",
       " 'thrilling': 279,\n",
       " 'short': 3842,\n",
       " 'critically': 61,\n",
       " 'acclaimed': 145,\n",
       " 'psychological': 515,\n",
       " 'based': 2862,\n",
       " 'true': 4519,\n",
       " 'events': 1723,\n",
       " 'gabriel': 166,\n",
       " 'celebrated': 98,\n",
       " 'late': 2375,\n",
       " 'talk': 1728,\n",
       " 'host': 342,\n",
       " 'becomes': 2648,\n",
       " 'captivated': 93,\n",
       " 'harrowing': 107,\n",
       " 'listener': 37,\n",
       " 'adoptive': 28,\n",
       " 'mother': 3211,\n",
       " 'toni': 118,\n",
       " 'collette': 57,\n",
       " 'troubling': 40,\n",
       " 'questions': 963,\n",
       " 'arise': 68,\n",
       " 'finds': 1791,\n",
       " 'drawn': 766,\n",
       " 'into': 17849,\n",
       " 'widening': 7,\n",
       " 'hides': 122,\n",
       " 'deadly': 393,\n",
       " 'secret': 1161,\n",
       " 'according': 558,\n",
       " 'official': 211,\n",
       " 'synopsis': 222,\n",
       " 'really': 23094,\n",
       " 'stop': 2284,\n",
       " 'reading': 1332,\n",
       " 'these': 10724,\n",
       " 'comments': 1535,\n",
       " 'now': 9262,\n",
       " 'lose': 650,\n",
       " 'ending': 4755,\n",
       " 'ms': 683,\n",
       " 'planning': 231,\n",
       " 'chopped': 118,\n",
       " 'sent': 769,\n",
       " 'deleted': 138,\n",
       " 'land': 821,\n",
       " 'overkill': 38,\n",
       " 'nature': 1313,\n",
       " 'physical': 619,\n",
       " 'mental': 563,\n",
       " 'ailments': 10,\n",
       " 'obvious': 2058,\n",
       " 'mr': 2874,\n",
       " 'returns': 663,\n",
       " 'york': 1420,\n",
       " 'possibly': 1344,\n",
       " 'blindness': 28,\n",
       " 'question': 1326,\n",
       " 'revelation': 269,\n",
       " 'certain': 1588,\n",
       " 'highway': 115,\n",
       " 'video': 3462,\n",
       " 'tape': 538,\n",
       " 'would': 24602,\n",
       " 'benefit': 218,\n",
       " 'editing': 1530,\n",
       " 'bobby': 299,\n",
       " 'cannavale': 11,\n",
       " 'jess': 120,\n",
       " 'initially': 351,\n",
       " 'believable': 1469,\n",
       " 'couple': 3465,\n",
       " 'establishing': 105,\n",
       " 'relationship': 1931,\n",
       " 'might': 5794,\n",
       " 'helped': 660,\n",
       " 'set': 4809,\n",
       " 'stage': 1295,\n",
       " 'otherwise': 1328,\n",
       " 'exemplary': 31,\n",
       " 'offers': 687,\n",
       " 'exceptionally': 168,\n",
       " 'strong': 2116,\n",
       " 'characterization': 247,\n",
       " 'gay': 1348,\n",
       " 'impersonation': 61,\n",
       " 'anna': 462,\n",
       " 'joe': 1253,\n",
       " 'morton': 59,\n",
       " 'ashe': 2,\n",
       " 'pete': 174,\n",
       " 'logand': 4,\n",
       " 'perfect': 3154,\n",
       " 'donna': 174,\n",
       " 'belongs': 282,\n",
       " 'creepy': 1216,\n",
       " 'hall': 473,\n",
       " 'correct': 449,\n",
       " 'saying': 1953,\n",
       " 'psycho': 475,\n",
       " 'several': 2857,\n",
       " 'organizations': 33,\n",
       " 'giving': 1638,\n",
       " 'awards': 437,\n",
       " 'seemed': 2703,\n",
       " 'reach': 493,\n",
       " 'women': 3562,\n",
       " 'due': 1783,\n",
       " 'slighter': 5,\n",
       " 'dispersion': 1,\n",
       " 'roles': 2094,\n",
       " 'certainly': 2969,\n",
       " 'noticed': 576,\n",
       " 'award': 814,\n",
       " 'consideration': 118,\n",
       " 'patrick': 427,\n",
       " 'stettner': 8,\n",
       " 'evokes': 97,\n",
       " 'hitchcock': 751,\n",
       " 'makes': 8312,\n",
       " 'getting': 3315,\n",
       " 'sandwich': 50,\n",
       " 'vending': 7,\n",
       " 'machine': 717,\n",
       " 'suspenseful': 394,\n",
       " 'finally': 3003,\n",
       " 'writers': 1260,\n",
       " 'armistead': 5,\n",
       " 'maupin': 7,\n",
       " 'terry': 225,\n",
       " 'anderson': 431,\n",
       " 'deserve': 601,\n",
       " 'gratitude': 31,\n",
       " 'attendants': 13,\n",
       " 'everywhere': 387,\n",
       " '21': 120,\n",
       " '06': 33,\n",
       " '2006': 288,\n",
       " '2': 5831,\n",
       " 'john': 4238,\n",
       " 'cullum': 3,\n",
       " 'lisa': 348,\n",
       " 'emery': 20,\n",
       " 'becky': 62,\n",
       " 'baker': 243,\n",
       " 'dir': 59,\n",
       " 'hitchcockian': 41,\n",
       " 'suspenser': 4,\n",
       " 'gives': 3070,\n",
       " 'stand': 1552,\n",
       " 'low': 3580,\n",
       " 'key': 818,\n",
       " 'celebrities': 86,\n",
       " 'fans': 2763,\n",
       " 'near': 1652,\n",
       " 'paranoia': 168,\n",
       " 'associates': 68,\n",
       " 'why': 10501,\n",
       " 'almost': 6260,\n",
       " 'norm': 121,\n",
       " 'latest': 411,\n",
       " 'derange': 2,\n",
       " 'fan': 3920,\n",
       " 'scenario': 357,\n",
       " 'no': 25292,\n",
       " 'radio': 725,\n",
       " 'personality': 674,\n",
       " 'named': 1565,\n",
       " 'reads': 170,\n",
       " 'stories': 2151,\n",
       " 'penned': 77,\n",
       " 'airwaves': 17,\n",
       " 'accumulated': 15,\n",
       " 'interesting': 6180,\n",
       " 'form': 1486,\n",
       " 'submitted': 45,\n",
       " 'manuscript': 45,\n",
       " 'travails': 17,\n",
       " 'troubled': 288,\n",
       " 'youth': 518,\n",
       " 'editor': 258,\n",
       " 'read': 3791,\n",
       " 'naturally': 543,\n",
       " 'disturbed': 218,\n",
       " 'ultimately': 976,\n",
       " 'intrigued': 215,\n",
       " 'nightmarish': 86,\n",
       " 'existence': 491,\n",
       " 'abducted': 54,\n",
       " 'sexually': 262,\n",
       " 'abused': 159,\n",
       " 'rescued': 116,\n",
       " 'nurse': 390,\n",
       " 'excellent': 4104,\n",
       " 'adopted': 147,\n",
       " 'correspondence': 19,\n",
       " 'reveals': 349,\n",
       " 'dying': 634,\n",
       " 'aids': 163,\n",
       " 'meet': 1328,\n",
       " 'suddenly': 1024,\n",
       " 'doubt': 1523,\n",
       " 'devious': 63,\n",
       " 'ulterior': 22,\n",
       " 'motives': 214,\n",
       " 'seed': 125,\n",
       " 'planted': 59,\n",
       " 'estranged': 103,\n",
       " 'lover': 758,\n",
       " 'whose': 1905,\n",
       " 'sudden': 441,\n",
       " 'departure': 115,\n",
       " 'city': 2371,\n",
       " 'apartment': 637,\n",
       " 'emotional': 1345,\n",
       " 'tailspin': 3,\n",
       " 'grown': 493,\n",
       " 'tempest': 69,\n",
       " 'teacup': 5,\n",
       " 'decides': 1046,\n",
       " 'investigating': 150,\n",
       " 'backgrounds': 178,\n",
       " 'discovering': 168,\n",
       " 'truths': 78,\n",
       " 'didn': 8768,\n",
       " 'anticipate': 37,\n",
       " 'co': 1121,\n",
       " 'wrote': 1099,\n",
       " 'screenplay': 1301,\n",
       " 'former': 1028,\n",
       " 'novice': 42,\n",
       " 'hoax': 26,\n",
       " 'run': 2503,\n",
       " 'full': 3552,\n",
       " 'tilt': 25,\n",
       " 'any': 15044,\n",
       " 'old': 8651,\n",
       " 'fashioned': 269,\n",
       " 'pot': 185,\n",
       " 'boiler': 25,\n",
       " 'helps': 709,\n",
       " 'conflicted': 79,\n",
       " 'hearted': 427,\n",
       " 'genuinely': 502,\n",
       " 'number': 1954,\n",
       " 'fact': 6902,\n",
       " 'him': 17588,\n",
       " 'thing': 9173,\n",
       " 'escaped': 237,\n",
       " 'own': 6594,\n",
       " 'unsettling': 178,\n",
       " 'dreadful': 493,\n",
       " 'trait': 67,\n",
       " 'leave': 2145,\n",
       " 'unmentioned': 6,\n",
       " 'underlines': 20,\n",
       " 'desperation': 177,\n",
       " 'rattle': 18,\n",
       " 'runs': 939,\n",
       " 'gas': 348,\n",
       " 'eventually': 1410,\n",
       " 'repetitive': 254,\n",
       " 'predictable': 1687,\n",
       " 'despite': 2694,\n",
       " 'finely': 62,\n",
       " 'directed': 2383,\n",
       " 'piece': 3047,\n",
       " 'hoodwink': 4,\n",
       " 'pays': 209,\n",
       " 'listen': 656,\n",
       " 'inner': 388,\n",
       " 'voice': 2250,\n",
       " 'careful': 189,\n",
       " 'hope': 2914,\n",
       " 'god': 2468,\n",
       " 'bless': 83,\n",
       " 'constantly': 830,\n",
       " 'shooting': 935,\n",
       " 'foot': 438,\n",
       " 'lately': 184,\n",
       " 'dumb': 1239,\n",
       " 'done': 6153,\n",
       " 'decade': 493,\n",
       " 'perhaps': 3293,\n",
       " 'exception': 735,\n",
       " 'death': 3945,\n",
       " 'smoochy': 3,\n",
       " 'bombed': 82,\n",
       " 'came': 3310,\n",
       " 'cult': 921,\n",
       " 'dramas': 286,\n",
       " 'especially': 5004,\n",
       " 'insomnia': 92,\n",
       " 'hour': 2351,\n",
       " 'photo': 172,\n",
       " 'mediocre': 689,\n",
       " 'reviews': 1444,\n",
       " 'quick': 696,\n",
       " 'dvd': 4936,\n",
       " 'release': 1624,\n",
       " 'among': 1567,\n",
       " 'period': 1472,\n",
       " 'chilling': 324,\n",
       " 'include': 740,\n",
       " 'serial': 692,\n",
       " 'killer': 2640,\n",
       " 'anyone': 5376,\n",
       " 'physically': 292,\n",
       " 'dangerous': 611,\n",
       " 'concept': 1008,\n",
       " 'actual': 1486,\n",
       " 'case': 3003,\n",
       " 'fraud': 83,\n",
       " 'yet': 5479,\n",
       " 'officially': 86,\n",
       " 'confirmed': 81,\n",
       " 'autobiography': 75,\n",
       " 'child': 2464,\n",
       " 'anthony': 513,\n",
       " 'godby': 1,\n",
       " 'johnson': 402,\n",
       " 'suffered': 287,\n",
       " 'horrific': 305,\n",
       " 'abuse': 395,\n",
       " 'contracted': 20,\n",
       " 'result': 1262,\n",
       " 'moved': 650,\n",
       " 'reports': 120,\n",
       " 'online': 163,\n",
       " 'may': 6619,\n",
       " 'exist': 553,\n",
       " 'confused': 741,\n",
       " 'feelings': 797,\n",
       " 'brilliantly': 469,\n",
       " 'portrayed': 1190,\n",
       " 'resurfaced': 10,\n",
       " 'mind': 3985,\n",
       " 'sociopathic': 18,\n",
       " 'caretaker': 59,\n",
       " 'role': 6291,\n",
       " 'cry': 767,\n",
       " 'little': 12435,\n",
       " 'miss': 1767,\n",
       " 'sunshine': 165,\n",
       " 'times': 6359,\n",
       " 'looked': 2077,\n",
       " 'camera': 3598,\n",
       " 'thought': 6937,\n",
       " 'staring': 191,\n",
       " 'takes': 4265,\n",
       " 'play': 4533,\n",
       " 'sort': 2938,\n",
       " 'understated': 186,\n",
       " 'reviewed': 106,\n",
       " 'actresses': 715,\n",
       " 'generation': 558,\n",
       " 'nominated': 443,\n",
       " 'academy': 568,\n",
       " '2008': 123,\n",
       " 'incredible': 1080,\n",
       " 'least': 6105,\n",
       " 'scary': 1858,\n",
       " 'dark': 2772,\n",
       " 'recommend': 3402,\n",
       " 'prepared': 382,\n",
       " 'unsettled': 18,\n",
       " 'because': 17715,\n",
       " 'leaves': 1348,\n",
       " 'strange': 1810,\n",
       " 'feeling': 2205,\n",
       " 'first': 17583,\n",
       " 'maupins': 1,\n",
       " 'taken': 2030,\n",
       " 'displayed': 191,\n",
       " 'cares': 471,\n",
       " 'loves': 816,\n",
       " 'said': 4307,\n",
       " 'we': 21261,\n",
       " 'version': 4111,\n",
       " 'expected': 1400,\n",
       " 'past': 2427,\n",
       " 'gloss': 43,\n",
       " 'hollywood': 3683,\n",
       " 'succeeded': 204,\n",
       " 'amount': 1017,\n",
       " 'restraint': 80,\n",
       " 'captures': 401,\n",
       " 'fragile': 100,\n",
       " 'essence': 278,\n",
       " 'lets': 689,\n",
       " 'us': 7380,\n",
       " 'struggle': 621,\n",
       " 'issues': 890,\n",
       " 'trust': 607,\n",
       " 'personnel': 55,\n",
       " 'around': 7154,\n",
       " 'introduced': 624,\n",
       " 'players': 555,\n",
       " 'reminded': 686,\n",
       " 'nothing': 8374,\n",
       " 'seems': 7076,\n",
       " 'smallest': 70,\n",
       " 'event': 697,\n",
       " 'change': 1858,\n",
       " 'our': 5021,\n",
       " 'lives': 2699,\n",
       " 'irrevocably': 9,\n",
       " 'request': 83,\n",
       " 'review': 1695,\n",
       " 'book': 4757,\n",
       " 'turns': 2415,\n",
       " 'changing': 382,\n",
       " 'find': 8219,\n",
       " 'strength': 489,\n",
       " 'within': 1620,\n",
       " 'carry': 644,\n",
       " 'forward': 1287,\n",
       " 'bad': 18473,\n",
       " 'avoid': 1527,\n",
       " 'average': 1426,\n",
       " 'american': 4580,\n",
       " 'serious': 2052,\n",
       " 'please': 2067,\n",
       " 'chance': 2114,\n",
       " 'touches': 385,\n",
       " 'darkness': 433,\n",
       " 'go': 9963,\n",
       " 'ourselves': 289,\n",
       " 'stepped': 66,\n",
       " 'another': 8593,\n",
       " 'quality': 2614,\n",
       " 'forget': 1457,\n",
       " 'steals': 444,\n",
       " '1940': 202,\n",
       " 'leading': 1192,\n",
       " 'looks': 4525,\n",
       " 'screen': 5040,\n",
       " 'presence': 847,\n",
       " 'hacks': 51,\n",
       " 'opinion': 1861,\n",
       " 'liked': 2944,\n",
       " 'action': 6497,\n",
       " 'tense': 312,\n",
       " 'opening': 1988,\n",
       " 'semi': 411,\n",
       " 'truck': 416,\n",
       " 'transitional': 10,\n",
       " 'filmed': 1619,\n",
       " 'ways': 1586,\n",
       " 'lapse': 42,\n",
       " 'photography': 851,\n",
       " 'unusual': 586,\n",
       " 'colors': 372,\n",
       " 'evil': 2699,\n",
       " 'd': 5956,\n",
       " '8': 1687,\n",
       " 'illnesses': 15,\n",
       " 'born': 762,\n",
       " ...}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# khs code : make_hash_map(df) 내에 map_book(hash_map, tokens) 포함\n",
    "def make_hash_map(df):\n",
    "    # Create a hash map.\n",
    "    hash_map = {}\n",
    "\n",
    "    # Iterate over each row in the DataFrame.\n",
    "    for index, row in df.iterrows():\n",
    "    # 문장에서 단어들을 추출한다.\n",
    "        words = extract_words(row['text'])\n",
    "\n",
    "        # hash map에 단어를 추가한다\n",
    "        for word in words:\n",
    "            if word in hash_map:\n",
    "                hash_map[word] = hash_map[word] + 1\n",
    "            else:\n",
    "                hash_map[word] = 1\n",
    "    return hash_map\n",
    "\n",
    "make_hash_map(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# frequent_vocab 함수를 다음과 같은 입력으로 정의하십시오 : word_freq 및 max_features\n",
    "def frequent_vocab(word_freq, max_features): \n",
    "    counter = 0  # 값 0으로 카운터를 초기화하십시오\n",
    "    vocab = []   # Vocab이라는 빈 목록을 만듭니다\n",
    "    # 단어를 빈도수가 낮은 순서로 사전에 나열합니다\n",
    "    for key, value in sorted(word_freq.items(), key= lambda item: (item[1], item[0]), reverse=True): \n",
    "       # 상위(max_features) 단어 수를 얻기 위한 루프 함수\n",
    "        if counter < max_features: \n",
    "            vocab.append(key)\n",
    "            counter += 1\n",
    "        else: break\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 가방을 포함하도록 지정된 차원이있는 숫자 배열을 설정합니다.\n",
    "hash_map = make_hash_map(df)\n",
    "vocab = frequent_vocab(hash_map, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 500)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 다음과 같은 입력으로 함수 bagofwords를 정의 - 벡터화: sentence, words\n",
    "def bagofwords(sentence, words):\n",
    "    sentence_words = extract_words(sentence) # 문장/트윗을 토큰화하고 변수 sentence_words에 할당\n",
    "    # 빈도 단어 수\n",
    "    bag = np.zeros(len(words)) # 크기가 len(words)이고 0으로 구성된 NumPy 배열 생성\n",
    "    # 트윗에 토큰이 있을 때 데이터를 반복하고 1의 값을 추가\n",
    "    for sw in sentence_words:\n",
    "        for i,word in enumerate(words):\n",
    "            if word == sw: \n",
    "                bag[i] += 1\n",
    "                \n",
    "    return np.array(bag) # 하나의 평가에 대한 단어 가방 반환\n",
    "\n",
    "n_words = len(vocab)\n",
    "n_docs = len(df)\n",
    "bag_o = np.zeros([n_docs,n_words])\n",
    "# 루프 함수를 사용하여 각 트윗에 대해 새 행을 추가합니다.\n",
    "for ii in range(n_docs): \n",
    "    # 이전 함수 'bagofwords'를 호출합니다. 입력을 참조하십시오 : sentence, words\n",
    "    bag_o[ii,:] = bagofwords(df['text'].iloc[ii], vocab)\n",
    "    \n",
    "# your code here\n",
    "bag_o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.25521225, 1.13990115, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.25521225, 0.22798023, 0.09884843, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.25521225, 0.68394069, 0.09884843, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.63803062, 0.        , 0.19769686, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.76563675, 0.22798023, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.63803062, 1.36788138, 0.88963589, ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 트윗 수(numdocs)와 토큰/워드 수(numwords)를 나타내는 변수 2개 초기화 (10860, 500)\n",
    "numdocs, numwords = np.shape(bag_o)\n",
    "# 위와 같이 TFIDF 수식으로 변경\n",
    "N = numdocs\n",
    "word_frequency = np.empty(numwords)\n",
    "\n",
    "# 단어가 나타나는 문서 수를 계산\n",
    "for word in range(numwords):\n",
    "    word_frequency[word]=np.sum((bag_o[:,word]>0)) # 각 행(트윗수)에 word드가 포함된 수 함께\n",
    "\n",
    "idf = np.log(N/word_frequency)\n",
    "\n",
    "# 초기화 tfidf 배열\n",
    "tfidf = np.empty([numdocs, numwords])\n",
    "\n",
    "# 트윗에서 반복, 용어 빈도(단어 가방으로 표시)를 idf로 곱합니다.\n",
    "for doc in range(numdocs):    \n",
    "    tfidf[doc, :] = bag_o[doc, :]*idf\n",
    "\n",
    "tfidf[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def classify(rf, X_all, y_all): # 훈련되지 않은 모델, tfidf 배열 및 훈련 대상의 값을 가져온다.\n",
    "    X_train,X_test,y_train,y_test = train_test_split(X_all,y_all,shuffle=True) # 무작위로 두 개를 나누어 학습과 테스트 세트를 만든다.\n",
    "    rf.fit(X_train,y_train) # 학습 세트에 모델을 맞춘다.\n",
    "    print(rf.score(X_test,y_test)) # 테스트 세트에 정확도 점수를 출력한다.\n",
    "    return rf # 훈련된 모델을 반환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.84184\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(solver = 'newton-cg')\n",
    "\n",
    "X_all = tfidf\n",
    "y_all = df['positive'].values\n",
    "logreg = classify(logreg, X_all, y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sentiment_predictor(sentiment):\n",
    "    # your code here\n",
    "    word_vector = bagofwords(sentiment, vocab) # 단어 가방 변수를 설정합니다.bagofwords 함수를 기억하십니까?\n",
    "    word_tfidf = word_vector*idf # tfidf값 찾기\n",
    "       \n",
    "    prediction = logreg.predict(word_tfidf.reshape(1, -1)) # 트윗이 자연재해와 관련이 있는지 없는지 예측\n",
    "    results = {1:'positive', 0:'negative'} # 잠재적인 결과를 포함하는 집합을 만듭니다.\"Relevant\" 및 \"Not relevant\" 태그를 변경할 수 있습니다.\n",
    "    # 결과 출력\n",
    "    print('The sentence is predicted as:', results[prediction[0]])   \n",
    "    #print('The sentence is predicted as:', results[prediction[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentence is predicted as: negative\n",
      "The sentence is predicted as: positive\n",
      "The sentence is predicted as: negative\n",
      "The sentence is predicted as: negative\n"
     ]
    }
   ],
   "source": [
    "# test_sentence = \"The movie was absolutely fantastic! The storyline was captivating, the acting was superb, and the visuals were stunning.\"\n",
    "# test_sentence = \"I loved every minute of the movie. The characters were relatable, the humor was on point, and the ending was satisfying.\"\n",
    "# test_sentence = \"I was disappointed with the movie. The plot was confusing, the acting felt forced, and the pacing was sluggish.\"\n",
    "# est_sentence = \"Unfortunately, the movie didn't live up to my expectations. The dialogue was uninspiring, the special effects were underwhelming, and the ending felt abrupt.\"\n",
    "# bagofwords(test_sentence, vocab)\n",
    "# predictor(test_sentence)\n",
    "\n",
    "test_sentence =[\"The movie was absolutely fantastic! The storyline was captivating, the acting was superb, and the visuals were stunning.\",\n",
    "                \"I loved every minute of the movie. The characters were relatable, the humor was on point, and the ending was satisfying.\",\n",
    "                \"I was disappointed with the movie. The plot was confusing, the acting felt forced, and the pacing was sluggish.\",\n",
    "                \"Unfortunately, the movie didn't live up to my expectations. The dialogue was uninspiring, the special effects were underwhelming, and the ending felt abrupt.\"]\n",
    "\n",
    "for test in test_sentence : \n",
    "    bagofwords(test, vocab)\n",
    "    sentiment_predictor(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sentiment_prediction(input_sentence):\n",
    "    # CountVectorizer와 TfidfTransformer를 이용하여 입력 문장 변환\n",
    "    input_data_features = vectorizer.transform([input_sentence])\n",
    "    input_tfidf = tfidfier.transform(input_data_features)\n",
    "\n",
    "    # 변환된 입력을 모델에 적용하여 감정 예측\n",
    "    prediction = lf.predict(input_tfidf)\n",
    "    results = {1: 'Positive', 0: 'Negative'} \n",
    "\n",
    "    # 결과 출력\n",
    "    print('The sentence is predicted as:', results[prediction[0]])\n",
    "\n",
    "test_sentence =[\n",
    "                \"The movie was absolutely fantastic! The storyline was captivating, the acting was superb, and the visuals were stunning.\",\n",
    "                \"I loved every minute of the movie. The characters were relatable, the humor was on point, and the ending was satisfying.\",\n",
    "                \"I was disappointed with the movie. The plot was confusing, the acting felt forced, and the pacing was sluggish.\",\n",
    "                \"Unfortunately, the movie didn't live up to my expectations. The dialogue was uninspiring, the special effects were underwhelming, and the ending felt abrupt.\"]\n",
    "\n",
    "for test in test_sentence : \n",
    "    sentiment_prediction(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## @방법3 : 라이브러리 사용하여 감정 분석 모델 만들고 테스트 - 간략화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model_Accuracy: 0.81704\n",
      "prediction_Accuracy: 0.88256\n"
     ]
    }
   ],
   "source": [
    "# khs code- type1\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def sentiment_predictor1(X_train, y_train, X_test, y_test):\n",
    "    global vectorizer\n",
    "    global tfidfier\n",
    "    global lf\n",
    "    \n",
    "    vectorizer = CountVectorizer(analyzer=\"word\", strip_accents=None, tokenizer=None, \\\n",
    "                                 preprocessor=None, stop_words=None, max_features=5000)\n",
    "    train_data_features = vectorizer.fit_transform(X_train)\n",
    "    test_data_features = vectorizer.transform(X_test)\n",
    "\n",
    "    tfidfier = TfidfTransformer()\n",
    "    train_tfidf = tfidfier.fit_transform(train_data_features)\n",
    "    test_tfidf = tfidfier.transform(test_data_features)\n",
    "\n",
    "    lf = LogisticRegression(solver = 'newton-cg', max_iter=200)\n",
    "    lf.fit(train_tfidf, y_train)\n",
    "    print('Model_Accuracy:' , lf.score(test_data_features, y_test))\n",
    "    \n",
    "    prediction = lf.predict(test_tfidf)       \n",
    "    results = {1: 'Positive', 0: 'Negative'}  \n",
    "    prediction_df = pd.DataFrame({'Predicted': prediction, 'Actual': y_test})\n",
    "    # print(prediction_df)\n",
    "\n",
    "    # Calculate the accuracy of the model\n",
    "    y_pred = np.mean(prediction_df['Predicted'] == prediction_df['Actual'])\n",
    "    print('prediction_Accuracy:', y_pred) \n",
    "    \n",
    "df_raw = pd.read_pickle('[Dataset]_Module25_df_raw.pkl')\n",
    "df_raw_test = pd.read_pickle('[Dataset]_Module25_df_raw_test.pkl')\n",
    "\n",
    "X_train = df_raw[\"text\"]\n",
    "y_train = df_raw['positive'].values\n",
    "X_test = df_raw_test[\"text\"]\n",
    "y_test = df_raw_test['positive'].values\n",
    "\n",
    "sentiment_predictor1(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model_Accuracy: 0.85144\n",
      "prediction_Accuracy: 0.80612\n"
     ]
    }
   ],
   "source": [
    "# khs code- type1\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def sentiment_predictor1(X_train, y_train, X_test, y_test):\n",
    "    global vectorizer\n",
    "    global tfidfier\n",
    "    global lf\n",
    "    \n",
    "    vectorizer = CountVectorizer(analyzer=\"word\", strip_accents=None, tokenizer=None, \\\n",
    "                                 preprocessor=None, stop_words=None, max_features=5000)\n",
    "    train_data_features = vectorizer.fit_transform(X_train)\n",
    "    test_data_features = vectorizer.transform(X_test)\n",
    "\n",
    "    tfidfier = TfidfTransformer()\n",
    "    tfidfier.fit(train_data_features)\n",
    "    test_tfidf = tfidfier.transform(test_data_features)\n",
    "\n",
    "    lf = LogisticRegression(solver = 'newton-cg', max_iter=200)\n",
    "    lf.fit(train_data_features, y_train)\n",
    "    print('Model_Accuracy:' , lf.score(test_data_features, y_test))\n",
    "    \n",
    "    prediction = lf.predict(test_tfidf)       \n",
    "    results = {1: 'Positive', 0: 'Negative'}  \n",
    "    prediction_df = pd.DataFrame({'Predicted': prediction, 'Actual': y_test})\n",
    "    # print(prediction_df)\n",
    "\n",
    "    # Calculate the accuracy of the model\n",
    "    y_pred = np.mean(prediction_df['Predicted'] == prediction_df['Actual'])\n",
    "    print('prediction_Accuracy:', y_pred) \n",
    "    \n",
    "df_raw = pd.read_pickle('[Dataset]_Module25_df_raw.pkl')\n",
    "df_raw_test = pd.read_pickle('[Dataset]_Module25_df_raw_test.pkl')\n",
    "\n",
    "X_train = df_raw[\"text\"]\n",
    "y_train = df_raw['positive'].values\n",
    "X_test = df_raw_test[\"text\"]\n",
    "y_test = df_raw_test['positive'].values\n",
    "\n",
    "sentiment_predictor1(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentence is predicted as: Positive\n",
      "The sentence is predicted as: Positive\n",
      "The sentence is predicted as: Negative\n",
      "The sentence is predicted as: Negative\n"
     ]
    }
   ],
   "source": [
    "def sentiment_prediction(input_sentence):\n",
    "    # CountVectorizer와 TfidfTransformer를 이용하여 입력 문장 변환\n",
    "    input_data_features = vectorizer.transform([input_sentence])\n",
    "    input_tfidf = tfidfier.transform(input_data_features)\n",
    "\n",
    "    # 변환된 입력을 모델에 적용하여 감정 예측\n",
    "    prediction = lf.predict(input_tfidf)\n",
    "    results = {1: 'Positive', 0: 'Negative'} \n",
    "\n",
    "    # 결과 출력\n",
    "    print('The sentence is predicted as:', results[prediction[0]])\n",
    "\n",
    "test_sentence =[\n",
    "                \"The movie was absolutely fantastic! The storyline was captivating, the acting was superb, and the visuals were stunning.\",\n",
    "                \"I loved every minute of the movie. The characters were relatable, the humor was on point, and the ending was satisfying.\",\n",
    "                \"I was disappointed with the movie. The plot was confusing, the acting felt forced, and the pacing was sluggish.\",\n",
    "                \"Unfortunately, the movie didn't live up to my expectations. The dialogue was uninspiring, the special effects were underwhelming, and the ending felt abrupt.\"]\n",
    "\n",
    "for test in test_sentence : \n",
    "    sentiment_prediction(test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
