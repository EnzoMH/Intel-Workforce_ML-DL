{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95bfea5e",
   "metadata": {},
   "source": [
    "# RNN을 이용하여 텍스트 생성하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46ac129",
   "metadata": {},
   "source": [
    "1. 데이터에 대한 이해와 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1208755e",
   "metadata": {},
   "source": [
    "1-1. 라이브러리 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cf1aa15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\isfs0\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\isfs0\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "C:\\Users\\isfs0\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences \n",
    "import numpy as np \n",
    "from tensorflow.keras.utils import to_categorical "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8750b8f",
   "metadata": {},
   "source": [
    "코드설명 - 라이브러리 : 주기능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364e2783",
   "metadata": {},
   "source": [
    "1. tensorflow.keras.preprocessing.text.Tokenizer:\n",
    "\n",
    "텍스트 데이터를 토큰화하고 단어를 정수로 매핑하는 데 사용됩니다.\n",
    "fit_on_texts(): 주어진 텍스트 데이터에 대해 단어 사전을 만듭니다.\n",
    "texts_to_sequences(): 텍스트 데이터를 정수 시퀀스로 변환합니다.\n",
    "word_index: 단어와 해당 정수 인덱스 사전을 확인할 수 있습니다.\n",
    "\n",
    "2. tensorflow.keras.preprocessing.sequence.pad_sequences:\n",
    "\n",
    "시퀀스 데이터의 패딩(padding)을 조절하는 데 사용됩니다.\n",
    "시퀀스의 길이를 맞추기 위해 0 또는 다른 값으로 시퀀스를 채웁니다.\n",
    "패딩된 시퀀스를 모델에 입력으로 사용할 수 있도록 합니다.\n",
    "\n",
    "3. numpy:\n",
    "\n",
    "다차원 배열과 행렬 연산을 수행하는 파이썬 라이브러리입니다.\n",
    "주로 데이터 배열을 다루는 데 사용됩니다.\n",
    "\n",
    "4. tensorflow.keras.utils.to_categorical:\n",
    "\n",
    "다중 클래스 분류(Classification) 문제에서 타겟(레이블)을 원-핫 인코딩 형식으로 변환하는 데 사용됩니다.\n",
    "각 클래스에 대한 이진 벡터로 변환하여 모델이 클래스 확률을 출력하기 쉽게 만듭니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13715246",
   "metadata": {},
   "source": [
    "1-2. 임의의 텍스트 데이터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27aeed42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 주어주기\n",
    "text=\"\"\"경마장에  있는  말이  뛰고  있다\\n 그의  말이  법이다\\n 가는  말이  고와야  오는  말이  곱다\\n\"\"\"     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b886cbc5",
   "metadata": {},
   "source": [
    "1-3. Tokenizer와 단어집합 사이즈 정해주기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bed9183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의  크기  : 12\n"
     ]
    }
   ],
   "source": [
    "t = Tokenizer() \n",
    "# Tokenizer 객체 t 생성, 텍스트 데이터 'text'를 토큰화하고\n",
    "# 단어를 정수로 매핑하는데에 사용\n",
    "\n",
    "t.fit_on_texts([text]) # Mapping 진행 \n",
    "# t를 사용하여 주어진 텍스트 데이터(text)를 토큰화하고 단어 집합을 생성합니다. \n",
    "# 여기서 text는 리스트 형태로 주어진 텍스트 데이터입니다.\n",
    "\n",
    "vocab_size = len(t.word_index) + 1 \n",
    "# 단어 집합의 크기를 계산\n",
    "# word_index 속성에는 단어와 해당 정수 인덱스의 매핑이 저장되어 있음\n",
    "# 따라서 이 속성의 길이에 1을 더하여 전체 단어 집합의 크기를 계산\n",
    "# * 1을 더하는 이유 : \n",
    "# 정수 인덱스가 1부터 시작하고 0은 보통 패딩용으로 예약되기 때문\n",
    "\n",
    "print('단어 집합의  크기  : %d' % vocab_size)\n",
    "# 계산된 단어 집합의 크기를 출력합니다. \n",
    "# 이 크기는 텍스트 데이터에서 고유한 단어의 수를 나타냅"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4838d50",
   "metadata": {},
   "source": [
    "1-4. t.word_index의 값 확인하기(집합의 크기는 12이므로, 집합의 요소들을 확인)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5dad86ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'말이': 1, '경마장에': 2, '있는': 3, '뛰고': 4, '있다': 5, '그의': 6, '법이다': 7, '가는': 8, '고와야': 9, '오는': 10, '곱다': 11}\n"
     ]
    }
   ],
   "source": [
    "print(t.word_index) \n",
    "# 각  단어와  단어에  부여된  정수  인덱스  출력 : ** 빈도수 순으로 출력\n",
    "# index를 사용하지만 0은 사용하지 않음 따라서 1~11 로 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1c53c5",
   "metadata": {},
   "source": [
    "'말이' 라는 단어가 출력되는 빈도가 가장 높으므로 인덱스값이 1로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faa795a",
   "metadata": {},
   "outputs": [],
   "source": [
    "1-5. sequence를 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36874f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 3, 1, 4, 5]]\n",
      "[[6, 1, 7]]\n",
      "[[8, 1, 9, 10, 1, 11]]\n",
      "[[]]\n",
      "학습에 사용한 샘플의 개수: 11\n"
     ]
    }
   ],
   "source": [
    "sequences = list()\n",
    "# 생성한 시퀀스를 저장할 리스트\n",
    "\n",
    "for line in text.split('\\n'):  \n",
    "    # 줄 바꿈(Wn)을 기준으로 문장을 분리\n",
    "    \n",
    "    print(t.texts_to_sequences([line]))  \n",
    "    # 현재 문장을 정수 시퀀스로 변환하여 출력\n",
    "    encoded = t.texts_to_sequences([line])[0]  \n",
    "    # texts_to_sequences 결과의 0번째 위치에 문장의 정수 시퀀스가 있음\n",
    "\n",
    "    # 문장의 일부를 사용하여 시퀀스 생성\n",
    "    for i in range(1, len(encoded)):\n",
    "        sequence = encoded[:i+1]\n",
    "        sequences.append(sequence)  # 생성한 시퀀스를 리스트에 추가\n",
    "\n",
    "print('학습에 사용한 샘플의 개수: %d' % len(sequences))  # 생성된 학습 샘플의 개수 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91280f8f",
   "metadata": {},
   "source": [
    "sequences = list(): 생성한 시퀀스(문장의 일부와 레이블)를 저장할 빈 리스트를 초기화합니다.\n",
    "\n",
    "for line in text.split('\\n'):: 입력 텍스트(text)를 줄 바꿈 문자(\\n)를 기준으로 문장으로 분리합니다.\n",
    "\n",
    "encoded = t.texts_to_sequences([line])[0]: 현재 문장을 Tokenizer 객체 t를 사용하여 정수 시퀀스로 변환하고, 이 정수 시퀀스를 encoded에 저장합니다.\n",
    "\n",
    "for i in range(1, len(encoded)):: 현재 문장의 정수 시퀀스를 이용하여 학습 샘플을 생성합니다. 이때, i를 1부터 encoded의 길이까지 변화시키면서, 각각의 시퀀스를 생성합니다.\n",
    "\n",
    "sequences.append(sequence): 생성한 시퀀스를 sequences 리스트에 추가합니다. 이 시퀀스는 문장의 일부와 그 다음에 오는 단어(레이블)로 구성됩니다.\n",
    "\n",
    "print('학습에 사용한 샘플의 개수: %d' % len(sequences)): 생성된 학습 샘플의 개수를 출력합니다. 이것은 학습에 사용할 수 있는 시퀀스 데이터의 총 개수를 나타냅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2ccb90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 3], [2, 3, 1], [2, 3, 1, 4], [2, 3, 1, 4, 5], [6, 1], [6, 1, 7], [8, 1], [8, 1, 9], [8, 1, 9, 10], [8, 1, 9, 10, 1], [8, 1, 9, 10, 1, 11]]\n"
     ]
    }
   ],
   "source": [
    "print(sequences) # 전체  샘플을  출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c87560b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "샘플의  최대  길이  : 6\n"
     ]
    }
   ],
   "source": [
    "max_len=max(len(l) for l in sequences) # 모든  샘플에서  길이가  가장  긴  샘플의  길이  출력 \n",
    "print('샘플의  최대  길이  : {}'.format(max_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23883993",
   "metadata": {},
   "source": [
    "1-5. 패딩 설정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8cf51dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0  2  3]\n",
      " [ 0  0  0  2  3  1]\n",
      " [ 0  0  2  3  1  4]\n",
      " [ 0  2  3  1  4  5]\n",
      " [ 0  0  0  0  6  1]\n",
      " [ 0  0  0  6  1  7]\n",
      " [ 0  0  0  0  8  1]\n",
      " [ 0  0  0  8  1  9]\n",
      " [ 0  0  8  1  9 10]\n",
      " [ 0  8  1  9 10  1]\n",
      " [ 8  1  9 10  1 11]]\n"
     ]
    }
   ],
   "source": [
    "# 전체체  샘플의  길이를  6으로  패딩 \n",
    "sequences = pad_sequences(sequences, maxlen=max_len, padding='pre') \n",
    "print(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9929cc08",
   "metadata": {},
   "source": [
    "1-6. 넘파이배열로 시퀀스를 저장, 학습과 정답데이터 쥐어주기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19751059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3  1  4  5  1  7  1  9 10  1 11]\n"
     ]
    }
   ],
   "source": [
    "sequences = np.array(sequences) \n",
    "\n",
    "X = sequences[:,:-1] # 학습  데이터 \n",
    "y = sequences[:,-1]  # 정답(Label) 데이터 print(X) \n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b588f4",
   "metadata": {},
   "source": [
    "1-7. 원핫인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da6d3b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "y = to_categorical(y, num_classes=vocab_size) # 원-핫  인코딩  수행 \n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3106c6b7",
   "metadata": {},
   "source": [
    "#### 2. 모델설계하기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c451f5ae",
   "metadata": {},
   "source": [
    "2-1. 라이브러리 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "243768be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Embedding, Dense, SimpleRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3192df",
   "metadata": {},
   "source": [
    "#### What is the Embedding?\n",
    "#### -> 단어를 고정된 차원의 벡터로 표현하는 기술"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee400e8",
   "metadata": {},
   "source": [
    "`tf.keras.layers.Embedding( input_dim, output_dim, embeddings_initializer='uniform', embeddings_regularizer=None, activity_regularizer=None, embeddings_constraint=None, mask_zero=False, input_length=None, **kwargs)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd6158f",
   "metadata": {},
   "source": [
    "- 인수\n",
    "    - input_dim : 입력 크기\n",
    "    - output_dim : 출력 크기\n",
    "    - input_length : 입력 데이터의 길이\n",
    "\n",
    "단어를 밀집벡터로 만드는 일을 수행한다. 정수 인코딩이 된 단어들을 입력으로 받아 수행한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2be0a52",
   "metadata": {},
   "source": [
    "2-2. 모델설정과 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53cc9610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 5, 6)              72        \n",
      "                                                                 \n",
      " simple_rnn (SimpleRNN)      (None, 32)                1248      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 12)                396       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1716 (6.70 KB)\n",
      "Trainable params: 1716 (6.70 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential() \n",
    "model.add(Embedding(vocab_size, 6, input_length=max_len-1)) \n",
    "model.add(SimpleRNN(32))  # SimpleRNN 오류발생하면 numpy 버전을 낮춤( pip install -U numpy==1.19.5) \n",
    "model.add(Dense(vocab_size, activation='softmax')) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d3a1a5",
   "metadata": {},
   "source": [
    "'model = Sequential()'\n",
    "- Sequential()을 사용하여 Sequential 모델을 생성합니다. \n",
    "- Sequential 모델은 레이어를 순차적으로 쌓는 데 사용됩니다.\n",
    "\n",
    "'model.add(Embedding(vocab_size, 6, input_length=max_len-1)) '\n",
    "- Embedding 레이어를 추가합니다. \n",
    "- 이 레이어는 단어 임베딩을 수행하는데 사용됩니다. \n",
    "- vocab_size는 어휘 사전의 크기를 나타내며, 6은 임베딩 차원의 크기를 나타냅니다. input_length는 입력 시퀀스의 길이를 나타냅니다.\n",
    "\n",
    "'model.add(SimpleRNN(32))'\n",
    "- SimpleRNN 레이어를 추가합니다. \n",
    "- 이 레이어는 32개의 유닛을 가지는 Simple RNN 레이어입니다.\n",
    "\n",
    "'model.add(Dense(vocab_size, activation='softmax'))' \n",
    "- Dense 레이어를 추가합니다. \n",
    "- 출력 레이어로 사용되며, vocab_size만큼의 뉴런을 가지며 \n",
    "- 활성화 함수로 소프트맥스(softmax)를 사용합니다. \n",
    "- 이것은 다중 클래스 분류 문제를 다루는데 적합합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51026f16",
   "metadata": {},
   "source": [
    "2-3. 모델컴파일 및 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d978c29c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1/1 - 1s - loss: 2.5054 - accuracy: 0.0909 - 1s/epoch - 1s/step\n",
      "Epoch 2/200\n",
      "1/1 - 0s - loss: 2.4957 - accuracy: 0.1818 - 4ms/epoch - 4ms/step\n",
      "Epoch 3/200\n",
      "1/1 - 0s - loss: 2.4863 - accuracy: 0.1818 - 3ms/epoch - 3ms/step\n",
      "Epoch 4/200\n",
      "1/1 - 0s - loss: 2.4771 - accuracy: 0.0909 - 4ms/epoch - 4ms/step\n",
      "Epoch 5/200\n",
      "1/1 - 0s - loss: 2.4680 - accuracy: 0.1818 - 0s/epoch - 0s/step\n",
      "Epoch 6/200\n",
      "1/1 - 0s - loss: 2.4589 - accuracy: 0.1818 - 0s/epoch - 0s/step\n",
      "Epoch 7/200\n",
      "1/1 - 0s - loss: 2.4497 - accuracy: 0.2727 - 5ms/epoch - 5ms/step\n",
      "Epoch 8/200\n",
      "1/1 - 0s - loss: 2.4402 - accuracy: 0.2727 - 5ms/epoch - 5ms/step\n",
      "Epoch 9/200\n",
      "1/1 - 0s - loss: 2.4304 - accuracy: 0.2727 - 4ms/epoch - 4ms/step\n",
      "Epoch 10/200\n",
      "1/1 - 0s - loss: 2.4202 - accuracy: 0.2727 - 0s/epoch - 0s/step\n",
      "Epoch 11/200\n",
      "1/1 - 0s - loss: 2.4094 - accuracy: 0.2727 - 0s/epoch - 0s/step\n",
      "Epoch 12/200\n",
      "1/1 - 0s - loss: 2.3980 - accuracy: 0.3636 - 17ms/epoch - 17ms/step\n",
      "Epoch 13/200\n",
      "1/1 - 0s - loss: 2.3860 - accuracy: 0.3636 - 0s/epoch - 0s/step\n",
      "Epoch 14/200\n",
      "1/1 - 0s - loss: 2.3732 - accuracy: 0.3636 - 13ms/epoch - 13ms/step\n",
      "Epoch 15/200\n",
      "1/1 - 0s - loss: 2.3596 - accuracy: 0.3636 - 0s/epoch - 0s/step\n",
      "Epoch 16/200\n",
      "1/1 - 0s - loss: 2.3450 - accuracy: 0.3636 - 0s/epoch - 0s/step\n",
      "Epoch 17/200\n",
      "1/1 - 0s - loss: 2.3295 - accuracy: 0.3636 - 17ms/epoch - 17ms/step\n",
      "Epoch 18/200\n",
      "1/1 - 0s - loss: 2.3130 - accuracy: 0.3636 - 0s/epoch - 0s/step\n",
      "Epoch 19/200\n",
      "1/1 - 0s - loss: 2.2953 - accuracy: 0.3636 - 13ms/epoch - 13ms/step\n",
      "Epoch 20/200\n",
      "1/1 - 0s - loss: 2.2766 - accuracy: 0.3636 - 609us/epoch - 609us/step\n",
      "Epoch 21/200\n",
      "1/1 - 0s - loss: 2.2566 - accuracy: 0.3636 - 0s/epoch - 0s/step\n",
      "Epoch 22/200\n",
      "1/1 - 0s - loss: 2.2355 - accuracy: 0.3636 - 3ms/epoch - 3ms/step\n",
      "Epoch 23/200\n",
      "1/1 - 0s - loss: 2.2133 - accuracy: 0.3636 - 0s/epoch - 0s/step\n",
      "Epoch 24/200\n",
      "1/1 - 0s - loss: 2.1899 - accuracy: 0.3636 - 15ms/epoch - 15ms/step\n",
      "Epoch 25/200\n",
      "1/1 - 0s - loss: 2.1655 - accuracy: 0.3636 - 0s/epoch - 0s/step\n",
      "Epoch 26/200\n",
      "1/1 - 0s - loss: 2.1404 - accuracy: 0.3636 - 0s/epoch - 0s/step\n",
      "Epoch 27/200\n",
      "1/1 - 0s - loss: 2.1145 - accuracy: 0.3636 - 4ms/epoch - 4ms/step\n",
      "Epoch 28/200\n",
      "1/1 - 0s - loss: 2.0884 - accuracy: 0.3636 - 0s/epoch - 0s/step\n",
      "Epoch 29/200\n",
      "1/1 - 0s - loss: 2.0623 - accuracy: 0.3636 - 14ms/epoch - 14ms/step\n",
      "Epoch 30/200\n",
      "1/1 - 0s - loss: 2.0366 - accuracy: 0.3636 - 0s/epoch - 0s/step\n",
      "Epoch 31/200\n",
      "1/1 - 0s - loss: 2.0119 - accuracy: 0.3636 - 0s/epoch - 0s/step\n",
      "Epoch 32/200\n",
      "1/1 - 0s - loss: 1.9885 - accuracy: 0.3636 - 3ms/epoch - 3ms/step\n",
      "Epoch 33/200\n",
      "1/1 - 0s - loss: 1.9670 - accuracy: 0.3636 - 0s/epoch - 0s/step\n",
      "Epoch 34/200\n",
      "1/1 - 0s - loss: 1.9475 - accuracy: 0.3636 - 5ms/epoch - 5ms/step\n",
      "Epoch 35/200\n",
      "1/1 - 0s - loss: 1.9300 - accuracy: 0.3636 - 0s/epoch - 0s/step\n",
      "Epoch 36/200\n",
      "1/1 - 0s - loss: 1.9145 - accuracy: 0.3636 - 14ms/epoch - 14ms/step\n",
      "Epoch 37/200\n",
      "1/1 - 0s - loss: 1.9003 - accuracy: 0.3636 - 0s/epoch - 0s/step\n",
      "Epoch 38/200\n",
      "1/1 - 0s - loss: 1.8869 - accuracy: 0.3636 - 0s/epoch - 0s/step\n",
      "Epoch 39/200\n",
      "1/1 - 0s - loss: 1.8736 - accuracy: 0.3636 - 2ms/epoch - 2ms/step\n",
      "Epoch 40/200\n",
      "1/1 - 0s - loss: 1.8599 - accuracy: 0.3636 - 0s/epoch - 0s/step\n",
      "Epoch 41/200\n",
      "1/1 - 0s - loss: 1.8455 - accuracy: 0.3636 - 17ms/epoch - 17ms/step\n",
      "Epoch 42/200\n",
      "1/1 - 0s - loss: 1.8302 - accuracy: 0.3636 - 0s/epoch - 0s/step\n",
      "Epoch 43/200\n",
      "1/1 - 0s - loss: 1.8141 - accuracy: 0.3636 - 12ms/epoch - 12ms/step\n",
      "Epoch 44/200\n",
      "1/1 - 0s - loss: 1.7973 - accuracy: 0.3636 - 803us/epoch - 803us/step\n",
      "Epoch 45/200\n",
      "1/1 - 0s - loss: 1.7800 - accuracy: 0.3636 - 0s/epoch - 0s/step\n",
      "Epoch 46/200\n",
      "1/1 - 0s - loss: 1.7625 - accuracy: 0.3636 - 3ms/epoch - 3ms/step\n",
      "Epoch 47/200\n",
      "1/1 - 0s - loss: 1.7450 - accuracy: 0.3636 - 0s/epoch - 0s/step\n",
      "Epoch 48/200\n",
      "1/1 - 0s - loss: 1.7274 - accuracy: 0.4545 - 17ms/epoch - 17ms/step\n",
      "Epoch 49/200\n",
      "1/1 - 0s - loss: 1.7099 - accuracy: 0.4545 - 0s/epoch - 0s/step\n",
      "Epoch 50/200\n",
      "1/1 - 0s - loss: 1.6925 - accuracy: 0.4545 - 12ms/epoch - 12ms/step\n",
      "Epoch 51/200\n",
      "1/1 - 0s - loss: 1.6749 - accuracy: 0.4545 - 1ms/epoch - 1ms/step\n",
      "Epoch 52/200\n",
      "1/1 - 0s - loss: 1.6572 - accuracy: 0.4545 - 0s/epoch - 0s/step\n",
      "Epoch 53/200\n",
      "1/1 - 0s - loss: 1.6391 - accuracy: 0.4545 - 4ms/epoch - 4ms/step\n",
      "Epoch 54/200\n",
      "1/1 - 0s - loss: 1.6205 - accuracy: 0.4545 - 0s/epoch - 0s/step\n",
      "Epoch 55/200\n",
      "1/1 - 0s - loss: 1.6015 - accuracy: 0.4545 - 14ms/epoch - 14ms/step\n",
      "Epoch 56/200\n",
      "1/1 - 0s - loss: 1.5820 - accuracy: 0.4545 - 2ms/epoch - 2ms/step\n",
      "Epoch 57/200\n",
      "1/1 - 0s - loss: 1.5619 - accuracy: 0.4545 - 0s/epoch - 0s/step\n",
      "Epoch 58/200\n",
      "1/1 - 0s - loss: 1.5414 - accuracy: 0.4545 - 16ms/epoch - 16ms/step\n",
      "Epoch 59/200\n",
      "1/1 - 0s - loss: 1.5206 - accuracy: 0.4545 - 0s/epoch - 0s/step\n",
      "Epoch 60/200\n",
      "1/1 - 0s - loss: 1.4994 - accuracy: 0.4545 - 14ms/epoch - 14ms/step\n",
      "Epoch 61/200\n",
      "1/1 - 0s - loss: 1.4781 - accuracy: 0.5455 - 0s/epoch - 0s/step\n",
      "Epoch 62/200\n",
      "1/1 - 0s - loss: 1.4568 - accuracy: 0.5455 - 0s/epoch - 0s/step\n",
      "Epoch 63/200\n",
      "1/1 - 0s - loss: 1.4355 - accuracy: 0.5455 - 5ms/epoch - 5ms/step\n",
      "Epoch 64/200\n",
      "1/1 - 0s - loss: 1.4143 - accuracy: 0.5455 - 0s/epoch - 0s/step\n",
      "Epoch 65/200\n",
      "1/1 - 0s - loss: 1.3934 - accuracy: 0.5455 - 13ms/epoch - 13ms/step\n",
      "Epoch 66/200\n",
      "1/1 - 0s - loss: 1.3727 - accuracy: 0.5455 - 0s/epoch - 0s/step\n",
      "Epoch 67/200\n",
      "1/1 - 0s - loss: 1.3523 - accuracy: 0.5455 - 0s/epoch - 0s/step\n",
      "Epoch 68/200\n",
      "1/1 - 0s - loss: 1.3323 - accuracy: 0.5455 - 5ms/epoch - 5ms/step\n",
      "Epoch 69/200\n",
      "1/1 - 0s - loss: 1.3126 - accuracy: 0.5455 - 0s/epoch - 0s/step\n",
      "Epoch 70/200\n",
      "1/1 - 0s - loss: 1.2933 - accuracy: 0.5455 - 16ms/epoch - 16ms/step\n",
      "Epoch 71/200\n",
      "1/1 - 0s - loss: 1.2743 - accuracy: 0.5455 - 0s/epoch - 0s/step\n",
      "Epoch 72/200\n",
      "1/1 - 0s - loss: 1.2557 - accuracy: 0.5455 - 14ms/epoch - 14ms/step\n",
      "Epoch 73/200\n",
      "1/1 - 0s - loss: 1.2374 - accuracy: 0.5455 - 0s/epoch - 0s/step\n",
      "Epoch 74/200\n",
      "1/1 - 0s - loss: 1.2195 - accuracy: 0.6364 - 13ms/epoch - 13ms/step\n",
      "Epoch 75/200\n",
      "1/1 - 0s - loss: 1.2019 - accuracy: 0.6364 - 1ms/epoch - 1ms/step\n",
      "Epoch 76/200\n",
      "1/1 - 0s - loss: 1.1845 - accuracy: 0.6364 - 0s/epoch - 0s/step\n",
      "Epoch 77/200\n",
      "1/1 - 0s - loss: 1.1673 - accuracy: 0.6364 - 4ms/epoch - 4ms/step\n",
      "Epoch 78/200\n",
      "1/1 - 0s - loss: 1.1503 - accuracy: 0.6364 - 0s/epoch - 0s/step\n",
      "Epoch 79/200\n",
      "1/1 - 0s - loss: 1.1333 - accuracy: 0.6364 - 5ms/epoch - 5ms/step\n",
      "Epoch 80/200\n",
      "1/1 - 0s - loss: 1.1165 - accuracy: 0.6364 - 10ms/epoch - 10ms/step\n",
      "Epoch 81/200\n",
      "1/1 - 0s - loss: 1.0997 - accuracy: 0.6364 - 2ms/epoch - 2ms/step\n",
      "Epoch 82/200\n",
      "1/1 - 0s - loss: 1.0829 - accuracy: 0.6364 - 0s/epoch - 0s/step\n",
      "Epoch 83/200\n",
      "1/1 - 0s - loss: 1.0662 - accuracy: 0.6364 - 4ms/epoch - 4ms/step\n",
      "Epoch 84/200\n",
      "1/1 - 0s - loss: 1.0496 - accuracy: 0.7273 - 0s/epoch - 0s/step\n",
      "Epoch 85/200\n",
      "1/1 - 0s - loss: 1.0331 - accuracy: 0.7273 - 15ms/epoch - 15ms/step\n",
      "Epoch 86/200\n",
      "1/1 - 0s - loss: 1.0168 - accuracy: 0.7273 - 4ms/epoch - 4ms/step\n",
      "Epoch 87/200\n",
      "1/1 - 0s - loss: 1.0006 - accuracy: 0.7273 - 8ms/epoch - 8ms/step\n",
      "Epoch 88/200\n",
      "1/1 - 0s - loss: 0.9847 - accuracy: 0.7273 - 1ms/epoch - 1ms/step\n",
      "Epoch 89/200\n",
      "1/1 - 0s - loss: 0.9691 - accuracy: 0.7273 - 0s/epoch - 0s/step\n",
      "Epoch 90/200\n",
      "1/1 - 0s - loss: 0.9537 - accuracy: 0.7273 - 16ms/epoch - 16ms/step\n",
      "Epoch 91/200\n",
      "1/1 - 0s - loss: 0.9386 - accuracy: 0.7273 - 0s/epoch - 0s/step\n",
      "Epoch 92/200\n",
      "1/1 - 0s - loss: 0.9237 - accuracy: 0.7273 - 12ms/epoch - 12ms/step\n",
      "Epoch 93/200\n",
      "1/1 - 0s - loss: 0.9090 - accuracy: 0.7273 - 3ms/epoch - 3ms/step\n",
      "Epoch 94/200\n",
      "1/1 - 0s - loss: 0.8944 - accuracy: 0.7273 - 0s/epoch - 0s/step\n",
      "Epoch 95/200\n",
      "1/1 - 0s - loss: 0.8801 - accuracy: 0.7273 - 6ms/epoch - 6ms/step\n",
      "Epoch 96/200\n",
      "1/1 - 0s - loss: 0.8658 - accuracy: 0.7273 - 0s/epoch - 0s/step\n",
      "Epoch 97/200\n",
      "1/1 - 0s - loss: 0.8518 - accuracy: 0.7273 - 15ms/epoch - 15ms/step\n",
      "Epoch 98/200\n",
      "1/1 - 0s - loss: 0.8379 - accuracy: 0.7273 - 0s/epoch - 0s/step\n",
      "Epoch 99/200\n",
      "1/1 - 0s - loss: 0.8243 - accuracy: 0.7273 - 13ms/epoch - 13ms/step\n",
      "Epoch 100/200\n",
      "1/1 - 0s - loss: 0.8109 - accuracy: 0.8182 - 0s/epoch - 0s/step\n",
      "Epoch 101/200\n",
      "1/1 - 0s - loss: 0.7977 - accuracy: 0.8182 - 0s/epoch - 0s/step\n",
      "Epoch 102/200\n",
      "1/1 - 0s - loss: 0.7848 - accuracy: 0.8182 - 5ms/epoch - 5ms/step\n",
      "Epoch 103/200\n",
      "1/1 - 0s - loss: 0.7721 - accuracy: 0.8182 - 0s/epoch - 0s/step\n",
      "Epoch 104/200\n",
      "1/1 - 0s - loss: 0.7596 - accuracy: 0.8182 - 15ms/epoch - 15ms/step\n",
      "Epoch 105/200\n",
      "1/1 - 0s - loss: 0.7473 - accuracy: 0.8182 - 0s/epoch - 0s/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 106/200\n",
      "1/1 - 0s - loss: 0.7353 - accuracy: 0.8182 - 14ms/epoch - 14ms/step\n",
      "Epoch 107/200\n",
      "1/1 - 0s - loss: 0.7234 - accuracy: 0.8182 - 0s/epoch - 0s/step\n",
      "Epoch 108/200\n",
      "1/1 - 0s - loss: 0.7117 - accuracy: 0.8182 - 13ms/epoch - 13ms/step\n",
      "Epoch 109/200\n",
      "1/1 - 0s - loss: 0.7003 - accuracy: 0.8182 - 0s/epoch - 0s/step\n",
      "Epoch 110/200\n",
      "1/1 - 0s - loss: 0.6890 - accuracy: 0.8182 - 13ms/epoch - 13ms/step\n",
      "Epoch 111/200\n",
      "1/1 - 0s - loss: 0.6780 - accuracy: 0.8182 - 0s/epoch - 0s/step\n",
      "Epoch 112/200\n",
      "1/1 - 0s - loss: 0.6672 - accuracy: 0.8182 - 0s/epoch - 0s/step\n",
      "Epoch 113/200\n",
      "1/1 - 0s - loss: 0.6566 - accuracy: 0.8182 - 17ms/epoch - 17ms/step\n",
      "Epoch 114/200\n",
      "1/1 - 0s - loss: 0.6461 - accuracy: 0.8182 - 0s/epoch - 0s/step\n",
      "Epoch 115/200\n",
      "1/1 - 0s - loss: 0.6358 - accuracy: 0.8182 - 0s/epoch - 0s/step\n",
      "Epoch 116/200\n",
      "1/1 - 0s - loss: 0.6256 - accuracy: 0.8182 - 599us/epoch - 599us/step\n",
      "Epoch 117/200\n",
      "1/1 - 0s - loss: 0.6156 - accuracy: 0.8182 - 0s/epoch - 0s/step\n",
      "Epoch 118/200\n",
      "1/1 - 0s - loss: 0.6058 - accuracy: 0.8182 - 15ms/epoch - 15ms/step\n",
      "Epoch 119/200\n",
      "1/1 - 0s - loss: 0.5961 - accuracy: 0.8182 - 0s/epoch - 0s/step\n",
      "Epoch 120/200\n",
      "1/1 - 0s - loss: 0.5866 - accuracy: 0.8182 - 0s/epoch - 0s/step\n",
      "Epoch 121/200\n",
      "1/1 - 0s - loss: 0.5773 - accuracy: 0.8182 - 4ms/epoch - 4ms/step\n",
      "Epoch 122/200\n",
      "1/1 - 0s - loss: 0.5680 - accuracy: 0.8182 - 0s/epoch - 0s/step\n",
      "Epoch 123/200\n",
      "1/1 - 0s - loss: 0.5589 - accuracy: 0.8182 - 13ms/epoch - 13ms/step\n",
      "Epoch 124/200\n",
      "1/1 - 0s - loss: 0.5499 - accuracy: 0.8182 - 0s/epoch - 0s/step\n",
      "Epoch 125/200\n",
      "1/1 - 0s - loss: 0.5410 - accuracy: 0.8182 - 0s/epoch - 0s/step\n",
      "Epoch 126/200\n",
      "1/1 - 0s - loss: 0.5322 - accuracy: 0.8182 - 3ms/epoch - 3ms/step\n",
      "Epoch 127/200\n",
      "1/1 - 0s - loss: 0.5236 - accuracy: 0.8182 - 0s/epoch - 0s/step\n",
      "Epoch 128/200\n",
      "1/1 - 0s - loss: 0.5150 - accuracy: 0.8182 - 17ms/epoch - 17ms/step\n",
      "Epoch 129/200\n",
      "1/1 - 0s - loss: 0.5066 - accuracy: 0.8182 - 0s/epoch - 0s/step\n",
      "Epoch 130/200\n",
      "1/1 - 0s - loss: 0.4982 - accuracy: 0.8182 - 13ms/epoch - 13ms/step\n",
      "Epoch 131/200\n",
      "1/1 - 0s - loss: 0.4900 - accuracy: 0.8182 - 0s/epoch - 0s/step\n",
      "Epoch 132/200\n",
      "1/1 - 0s - loss: 0.4819 - accuracy: 0.8182 - 0s/epoch - 0s/step\n",
      "Epoch 133/200\n",
      "1/1 - 0s - loss: 0.4738 - accuracy: 0.8182 - 17ms/epoch - 17ms/step\n",
      "Epoch 134/200\n",
      "1/1 - 0s - loss: 0.4659 - accuracy: 0.8182 - 0s/epoch - 0s/step\n",
      "Epoch 135/200\n",
      "1/1 - 0s - loss: 0.4581 - accuracy: 0.9091 - 14ms/epoch - 14ms/step\n",
      "Epoch 136/200\n",
      "1/1 - 0s - loss: 0.4504 - accuracy: 1.0000 - 0s/epoch - 0s/step\n",
      "Epoch 137/200\n",
      "1/1 - 0s - loss: 0.4427 - accuracy: 1.0000 - 0s/epoch - 0s/step\n",
      "Epoch 138/200\n",
      "1/1 - 0s - loss: 0.4352 - accuracy: 1.0000 - 4ms/epoch - 4ms/step\n",
      "Epoch 139/200\n",
      "1/1 - 0s - loss: 0.4277 - accuracy: 1.0000 - 0s/epoch - 0s/step\n",
      "Epoch 140/200\n",
      "1/1 - 0s - loss: 0.4204 - accuracy: 1.0000 - 14ms/epoch - 14ms/step\n",
      "Epoch 141/200\n",
      "1/1 - 0s - loss: 0.4131 - accuracy: 1.0000 - 0s/epoch - 0s/step\n",
      "Epoch 142/200\n",
      "1/1 - 0s - loss: 0.4059 - accuracy: 1.0000 - 13ms/epoch - 13ms/step\n",
      "Epoch 143/200\n",
      "1/1 - 0s - loss: 0.3988 - accuracy: 1.0000 - 2ms/epoch - 2ms/step\n",
      "Epoch 144/200\n",
      "1/1 - 0s - loss: 0.3918 - accuracy: 1.0000 - 0s/epoch - 0s/step\n",
      "Epoch 145/200\n",
      "1/1 - 0s - loss: 0.3849 - accuracy: 1.0000 - 3ms/epoch - 3ms/step\n",
      "Epoch 146/200\n",
      "1/1 - 0s - loss: 0.3781 - accuracy: 1.0000 - 0s/epoch - 0s/step\n",
      "Epoch 147/200\n",
      "1/1 - 0s - loss: 0.3714 - accuracy: 1.0000 - 16ms/epoch - 16ms/step\n",
      "Epoch 148/200\n",
      "1/1 - 0s - loss: 0.3647 - accuracy: 1.0000 - 0s/epoch - 0s/step\n",
      "Epoch 149/200\n",
      "1/1 - 0s - loss: 0.3582 - accuracy: 1.0000 - 13ms/epoch - 13ms/step\n",
      "Epoch 150/200\n",
      "1/1 - 0s - loss: 0.3517 - accuracy: 1.0000 - 0s/epoch - 0s/step\n",
      "Epoch 151/200\n",
      "1/1 - 0s - loss: 0.3453 - accuracy: 1.0000 - 0s/epoch - 0s/step\n",
      "Epoch 152/200\n",
      "1/1 - 0s - loss: 0.3390 - accuracy: 1.0000 - 2ms/epoch - 2ms/step\n",
      "Epoch 153/200\n",
      "1/1 - 0s - loss: 0.3328 - accuracy: 1.0000 - 0s/epoch - 0s/step\n",
      "Epoch 154/200\n",
      "1/1 - 0s - loss: 0.3267 - accuracy: 1.0000 - 15ms/epoch - 15ms/step\n",
      "Epoch 155/200\n",
      "1/1 - 0s - loss: 0.3206 - accuracy: 1.0000 - 0s/epoch - 0s/step\n",
      "Epoch 156/200\n",
      "1/1 - 0s - loss: 0.3147 - accuracy: 1.0000 - 13ms/epoch - 13ms/step\n",
      "Epoch 157/200\n",
      "1/1 - 0s - loss: 0.3088 - accuracy: 1.0000 - 2ms/epoch - 2ms/step\n",
      "Epoch 158/200\n",
      "1/1 - 0s - loss: 0.3030 - accuracy: 1.0000 - 0s/epoch - 0s/step\n",
      "Epoch 159/200\n",
      "1/1 - 0s - loss: 0.2973 - accuracy: 1.0000 - 17ms/epoch - 17ms/step\n",
      "Epoch 160/200\n",
      "1/1 - 0s - loss: 0.2916 - accuracy: 1.0000 - 0s/epoch - 0s/step\n",
      "Epoch 161/200\n",
      "1/1 - 0s - loss: 0.2861 - accuracy: 1.0000 - 12ms/epoch - 12ms/step\n",
      "Epoch 162/200\n",
      "1/1 - 0s - loss: 0.2806 - accuracy: 1.0000 - 0s/epoch - 0s/step\n",
      "Epoch 163/200\n",
      "1/1 - 0s - loss: 0.2752 - accuracy: 1.0000 - 0s/epoch - 0s/step\n",
      "Epoch 164/200\n",
      "1/1 - 0s - loss: 0.2699 - accuracy: 1.0000 - 4ms/epoch - 4ms/step\n",
      "Epoch 165/200\n",
      "1/1 - 0s - loss: 0.2647 - accuracy: 1.0000 - 0s/epoch - 0s/step\n",
      "Epoch 166/200\n",
      "1/1 - 0s - loss: 0.2596 - accuracy: 1.0000 - 14ms/epoch - 14ms/step\n",
      "Epoch 167/200\n",
      "1/1 - 0s - loss: 0.2545 - accuracy: 1.0000 - 0s/epoch - 0s/step\n",
      "Epoch 168/200\n",
      "1/1 - 0s - loss: 0.2496 - accuracy: 1.0000 - 0s/epoch - 0s/step\n",
      "Epoch 169/200\n",
      "1/1 - 0s - loss: 0.2447 - accuracy: 1.0000 - 17ms/epoch - 17ms/step\n",
      "Epoch 170/200\n",
      "1/1 - 0s - loss: 0.2399 - accuracy: 1.0000 - 0s/epoch - 0s/step\n",
      "Epoch 171/200\n",
      "1/1 - 0s - loss: 0.2352 - accuracy: 1.0000 - 0s/epoch - 0s/step\n",
      "Epoch 172/200\n",
      "1/1 - 0s - loss: 0.2305 - accuracy: 1.0000 - 2ms/epoch - 2ms/step\n",
      "Epoch 173/200\n",
      "1/1 - 0s - loss: 0.2260 - accuracy: 1.0000 - 0s/epoch - 0s/step\n",
      "Epoch 174/200\n",
      "1/1 - 0s - loss: 0.2215 - accuracy: 1.0000 - 15ms/epoch - 15ms/step\n",
      "Epoch 175/200\n",
      "1/1 - 0s - loss: 0.2171 - accuracy: 1.0000 - 0s/epoch - 0s/step\n",
      "Epoch 176/200\n",
      "1/1 - 0s - loss: 0.2128 - accuracy: 1.0000 - 13ms/epoch - 13ms/step\n",
      "Epoch 177/200\n",
      "1/1 - 0s - loss: 0.2086 - accuracy: 1.0000 - 0s/epoch - 0s/step\n",
      "Epoch 178/200\n",
      "1/1 - 0s - loss: 0.2044 - accuracy: 1.0000 - 0s/epoch - 0s/step\n",
      "Epoch 179/200\n",
      "1/1 - 0s - loss: 0.2003 - accuracy: 1.0000 - 5ms/epoch - 5ms/step\n",
      "Epoch 180/200\n",
      "1/1 - 0s - loss: 0.1964 - accuracy: 1.0000 - 0s/epoch - 0s/step\n",
      "Epoch 181/200\n",
      "1/1 - 0s - loss: 0.1925 - accuracy: 1.0000 - 14ms/epoch - 14ms/step\n",
      "Epoch 182/200\n",
      "1/1 - 0s - loss: 0.1886 - accuracy: 1.0000 - 0s/epoch - 0s/step\n",
      "Epoch 183/200\n",
      "1/1 - 0s - loss: 0.1849 - accuracy: 1.0000 - 12ms/epoch - 12ms/step\n",
      "Epoch 184/200\n",
      "1/1 - 0s - loss: 0.1812 - accuracy: 1.0000 - 2ms/epoch - 2ms/step\n",
      "Epoch 185/200\n",
      "1/1 - 0s - loss: 0.1776 - accuracy: 1.0000 - 0s/epoch - 0s/step\n",
      "Epoch 186/200\n",
      "1/1 - 0s - loss: 0.1741 - accuracy: 1.0000 - 4ms/epoch - 4ms/step\n",
      "Epoch 187/200\n",
      "1/1 - 0s - loss: 0.1706 - accuracy: 1.0000 - 8ms/epoch - 8ms/step\n",
      "Epoch 188/200\n",
      "1/1 - 0s - loss: 0.1673 - accuracy: 1.0000 - 5ms/epoch - 5ms/step\n",
      "Epoch 189/200\n",
      "1/1 - 0s - loss: 0.1640 - accuracy: 1.0000 - 0s/epoch - 0s/step\n",
      "Epoch 190/200\n",
      "1/1 - 0s - loss: 0.1607 - accuracy: 1.0000 - 0s/epoch - 0s/step\n",
      "Epoch 191/200\n",
      "1/1 - 0s - loss: 0.1576 - accuracy: 1.0000 - 2ms/epoch - 2ms/step\n",
      "Epoch 192/200\n",
      "1/1 - 0s - loss: 0.1545 - accuracy: 1.0000 - 0s/epoch - 0s/step\n",
      "Epoch 193/200\n",
      "1/1 - 0s - loss: 0.1515 - accuracy: 1.0000 - 15ms/epoch - 15ms/step\n",
      "Epoch 194/200\n",
      "1/1 - 0s - loss: 0.1486 - accuracy: 1.0000 - 0s/epoch - 0s/step\n",
      "Epoch 195/200\n",
      "1/1 - 0s - loss: 0.1457 - accuracy: 1.0000 - 12ms/epoch - 12ms/step\n",
      "Epoch 196/200\n",
      "1/1 - 0s - loss: 0.1429 - accuracy: 1.0000 - 2ms/epoch - 2ms/step\n",
      "Epoch 197/200\n",
      "1/1 - 0s - loss: 0.1401 - accuracy: 1.0000 - 0s/epoch - 0s/step\n",
      "Epoch 198/200\n",
      "1/1 - 0s - loss: 0.1375 - accuracy: 1.0000 - 16ms/epoch - 16ms/step\n",
      "Epoch 199/200\n",
      "1/1 - 0s - loss: 0.1349 - accuracy: 1.0000 - 0s/epoch - 0s/step\n",
      "Epoch 200/200\n",
      "1/1 - 0s - loss: 0.1323 - accuracy: 1.0000 - 12ms/epoch - 12ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1abfba41b80>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "model.fit(X, y, epochs=200, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fea7d96",
   "metadata": {},
   "source": [
    "#### RNN의 알고리즘방식 간접체험하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "927b51ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded =>  [2]\n",
      "result =>  [3]\n",
      "encoded =>  [2, 3]\n",
      "result =>  [1]\n",
      "encoded =>  [2, 3, 1]\n",
      "result =>  [4]\n",
      "encoded =>  [2, 3, 1, 4]\n",
      "result =>  [5]\n",
      "경마장에 있는 말이 뛰고 있다\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sentence_generation(model, t, current_word, n):\n",
    "    init_word = current_word  # 입력된 시작 단어를 저장\n",
    "    sentence = ''  # 생성된 문장을 저장할 변수\n",
    "\n",
    "    for _ in range(n):  # n번 반복하면서 단어를 예측하고 문장을 생성\n",
    "        encoded = t.texts_to_sequences([current_word])[0]  # 현재 단어에 대한 정수 인코딩을 수행\n",
    "        print(\"encoded => \", encoded)\n",
    "        encoded = pad_sequences([encoded], maxlen=5, padding='pre')  # 데이터에 패딩을 추가하여 입력 시퀀스 길이를 맞춤\n",
    "        result = model.predict(encoded, verbose=0)  # 모델을 사용하여 다음 단어를 예측\n",
    "        result = np.argmax(result, axis=-1)  # 가장 확률이 높은 단어의 인덱스를 선택\n",
    "        print(\"result => \", result)\n",
    "        \n",
    "        for word, index in t.word_index.items():  # 예측 결과를 단어로 변환\n",
    "            if index == result:\n",
    "                break\n",
    "        current_word = current_word + ' '  + word  # 현재 단어와 예측한 단어를 이어붙임\n",
    "        sentence = sentence + ' ' + word  # 예측한 단어를 문장에 추가\n",
    "\n",
    "    sentence = init_word + sentence  # 초기 단어와 생성된 문장을 합쳐 최종 문장 생성\n",
    "    return sentence\n",
    "\n",
    "# 함수를 호출하여 주어진 모델과 입력 단어 \"경마장에\"를 이용하여 4개의 단어를 생성하고 출력\n",
    "print(sentence_generation(model, t, '경마장에', 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ce62df",
   "metadata": {},
   "source": [
    "'model'\n",
    "- 텍스트 생성에 사용되는 Keras 모델입니다.\n",
    "\n",
    "'t'\n",
    "- 토크나이저 객체로, 단어와 정수 인덱스 간의 매핑을 가지고 있습니다.\n",
    "\n",
    "'current_word' \n",
    "- 시작 단어로, 문장 생성이 시작되는 단어입니다.\n",
    "\n",
    "'n' \n",
    "- 생성할 단어의 수를 나타내는 변수입니다.\n",
    "\n",
    "'encoded'\n",
    "- 현재 단어를 정수 시퀀스로 변환한 결과를 저장합니다.\n",
    "\n",
    "'pad_sequences'\n",
    "- 입력 시퀀스의 길이를 맞추기 위해 패딩을 추가합니다.\n",
    "\n",
    "'result'\n",
    "- 모델을 사용하여 다음 단어를 예측한 결과를 저장합니다.\n",
    "\n",
    "'t.word_index'\n",
    "- 토크나이저에서 단어와 인덱스 간의 매핑을 확인하기 위해 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd42c97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded =>  [2]\n",
      "result =>  [3]\n",
      "encoded =>  [2, 3]\n",
      "result =>  [1]\n",
      "encoded =>  [2, 3, 1]\n",
      "result =>  [4]\n",
      "encoded =>  [2, 3, 1, 4]\n",
      "result =>  [5]\n",
      "경마장에 있는 말이 뛰고 있다\n"
     ]
    }
   ],
   "source": [
    "print(sentence_generation(model, t, '경마장에', 4))  # 경마장에 다음에 나올 단어를 4개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71e8d854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded =>  [6]\n",
      "result =>  [1]\n",
      "encoded =>  [6, 1]\n",
      "result =>  [7]\n",
      "그의 말이 법이다\n",
      "encoded =>  [8]\n",
      "result =>  [1]\n",
      "encoded =>  [8, 1]\n",
      "result =>  [9]\n",
      "encoded =>  [8, 1, 9]\n",
      "result =>  [10]\n",
      "encoded =>  [8, 1, 9, 10]\n",
      "result =>  [1]\n",
      "encoded =>  [8, 1, 9, 10, 1]\n",
      "result =>  [11]\n",
      "가는 말이 고와야 오는 말이 곱다\n"
     ]
    }
   ],
   "source": [
    "print(sentence_generation(model, t, '그의', 2)) # 2번 예측 \n",
    "print(sentence_generation(model, t, '가는', 5)) # 5번 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba142ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
